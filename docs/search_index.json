[["selling-and-buying-process.html", "ASM_II_Class_Note Chapter 1 Selling and buying process: 1.1 Salespeople skill/ perception 1.2 Communication - intraorganization 1.3 Communication Interorganization - FLE interactions 1.4 Sales marketing interface 1.5 Firm level impact on salespeople 1.6 Salesperson trait orientation 1.7 Salesperson non-sales activity/ service 1.8 B2G selling", " ASM_II_Class_Note Yufan Lin 3/8/2022 Chapter 1 Selling and buying process: 1.1 Salespeople skill/ perception include the intuition/ first impression, price importance sensing. salespeople skill vs. adaptive selling, SOCO. (Spiro and Weitz 1990) Adaptive selling: conceptualization, measurement, and nomological validity Introduce the skill of adaptive selling. even thought all adapt, but to what extent is not clear. how effective is not clear. A 16-item scale is developed to measure the degree to which salespeople practice adaptive selling-the degree to which they alter their sales presentation across and during customer interactions in response to the perceived nature of the sales situ- ation. This paper-and-pencil scale assesses self-reports of five facets of adaptive selling: (1) recognition that different sales approaches are needed for different cus- tomers, (2) confidence in ability to use a variety of approaches, (3) confidence in ability to alter approach during an interaction, (4) collection of information to fa- cilitate adaptation, and (5) actual use of different approaches. The reliability of the scale is .85. Support for the nomological validity of the scale is found by failure to disconfirm relationships with an antecedent (intrinsic motivation), several general personality measures of interpersonal flexibility (self-monitoring, empathy, locus of control, and androgyny), and a consequence (self-reported perfo (Mullins et al. 2014) Know Your Customer: How Salesperson Perceptions of Customer Relationship Quality Form and Influence Account Profitability salespeoples skill in sense the customer impacts the account profit Firms often utilize salesperson intelligence in marketing strategies to improve sales performance. However, this approach is problematic if the information is based on inaccurate perceptions. In light of this, the authors introduce a theoretical model to study the antecedents and profit impact of salesperson perceptions of customer relationship quality. Dyadic analyses using matched survey responses from salespersoncustomer dyads and secondary performance data reveal several insightful findings. Results show that self-efficacious salespeople are upwardly biased, whereas customer-oriented salespeople are downwardly biased in their perceptions of customer relationship quality. However, managers can correct these inaccuracies using a behavior-based control system. Response surface analyses illustrate that the effects of salesperson accuracy and inaccuracy are distinct and curvilinear. During later relationship phases, salespeople profit more from salesperson accuracy in high- and lowquality relationships (i.e., a U-shaped effect). Yet the increasingly harmful impact of salesperson inaccuracy on profit is more severe during earlier relationship phases. Together, these findings highlight the benefits of measuring salesperson perceptions and how to manage them (Roman Avila and Iacobucci 2010) Antecedents and consequences of adaptive selling confidence and behavior: a dyadic analysis of salespeople and their customers antecdent and consequence of adaptive selling Personal selling is thought to be a very effective marketing vehicle. The notion of adaptive selling suggests that it should work better than any other means of communication because salespeople are able to develop a unique message for each customer. This research proposes a model of key antecedents and consequences of adaptive selling. In particular, we distinguish, measure, and model the attitudinal and behavioral aspects of adaptive selling, something that is encouraged but not thoroughly examined in the literature. Hypotheses are tested using data from 210 salesperson-customer dyads. The results indicate that a salespersons perception of the firms customer orientation has an effect on adaptive selling behavior through the salespersons adaptive selling confidence, role ambiguity, intrinsic motivation and customer-qualification skills. Adaptive selling behavior increases salespersons outcome performance, customers evaluations of satisfaction with the product and with the salesperson, which enhance customers anticipation of future interactions with the salesperson. The implications for management and theory are discussed. 1.2 Communication - intraorganization (Johlke et al. 2000) Sales managers impact on the salesperson job outcomes drawing from several diverse streams of research, the authors develop the rationale and empirical background for considering the role of sales manager communication practices. Using a multifaceted conceptualization of communication as its base, the study justifies, proposes, and evaluates a model describing the relations among sales managerscommunication practices and salesperson ambiguity, satisfaction, performance, and commitment. The results support the hypothesized model and suggest that sales manager communication practices are associated with these important salesperson job outcomes. (Bolander et al. 2015) Social Networks Within Sales Organizations: Their Development and Importance for Salesperson Performance Salespeoples communication among peers on their job performance Although the study of salesperson performance traditionally has focused on salespeoples activities and relationships with customers, scholars recently have proposed that salespeoples intraorganizational relationships and activities also play a vital role in driving sales performance. Using data from 286 salespeople in a unique social network analysis, the authors explore the effects of salespeoples intraorganizational relationships on objective salesperson performance as well as the role of political skill in developing intraorganizational relationships. The results indicate that two types of social network characteristics (i.e., relational centrality and positional centrality) contribute substantially to salesperson performance. Moreover, salespeoples political skill is shown to be an antecedent to relational centrality but, surprisingly, not positional centrality. This finding demonstrates that researchers should not assume that all centralities represent similar underlying network characteristics. In light of these results, the authors discuss several implications for both managers and researchers as well as directions for further research 1.3 Communication Interorganization - FLE interactions (S. K. Singh, n.d.) E-negotiations, or sales negotiations over email, are increasingly common in business-to-business (B2B) sales, but little is known about selling effectiveness in this medium. This research investigates salespeoples use of influence tactics as textual cues to manage buyers attention during B2B e-negotiations to win sales contract award. Drawing on studies of attention as a selection heuristic, the authors advance the literature on mechanisms of sales influence by theorizing buyer attention as a key mediating variable between the use of influence tactics and contract award. They use a unique, longitudinal panel spanning more than two years of email communications between buyers and salespeople during B2B sales negotiations to develop a validated corpus of textual cues that are diagnostic of salespeoples influence tactics in e-negotiations. These e-communications data are augmented by salesperson in-depth interviews and survey, archival performance data, and a controlled experimental study with professional salespeople. The obtained results indicate that the concurrent use of compliance or internalization-based tactics as textual cues bolsters buyers attention and is associated with greater likelihood of contract award. In contrast, concurrent use of compliance and internalization-based tactics is prone to degrade buyer attention and likely to put the salesperson at a disadvantage in closing the contract award. (S. Singh et al. 2017) Using a novel approach with video-recordings of sales interactions, this study focuses on a dynamic analysis of salesperson effectiveness in handling customer queries. We conceptualize salesperson behaviors, namely, resolving, relating, and emoting, as separate elements of customer query handling and empirically identify the distinct verbal and nonverbal cues that salespeople use to display these behaviors during sales interactions. We draw from compensation effects in social cognition theory to propose that customers perceptions of a salespersons effectiveness are prone to trade-offs between competence (resolving behaviors) and warmth (relating and emoting behaviors). Results, robust to endogeneity corrections, support the proposed tradeoffs such that the effectiveness of salespersons resolving behavior is significantly curtailed, even neutralized, by the salespersons relating and emoting behaviors. We situate these counterintuitive results within the extant theory and research on sales interactions, and outline implications for practice. (Boorom, Goolsby, and Ramsey 1998) Relational Communication Traits and Their Effect on Adaptiveness and Sales Performance How does the salesperson trait affects the communication effectiveness and then on the job performance. Two relational communication traits, communication apprehension and interaction involvement, are investigated within an adaptive selling framework to assess their impact on salesperson adaptiveness and sales performance. Using a sample of 239 insurance salespeople, results demonstrate that salespeople exhibiting lower levels of communication apprehension are more highly involved in communication interactions, and higher involvement facilitates increased adaptiveness and sales performance. This research highlights the importance of effective communication within sales interactions and offers suggestions to improve salesperson communication skill (Soldow and Thomas 1984) Relational Communication: Form versus Content in the Sales Interaction two type of f2f comm: content and relationship In view of the importance of interpersonal com- munication in the face-to-face selling interaction, this discussion seeks to provide a more complete picture of the actual communication process by introducing a concept new to the marketing liter- ature. The concept is relational communication, which refers to that part of a message beyond the actual content which allows communicators to ne- gotiate their relative positions. Thus, the message sender can either bid for dominance, deference, or equality. The message receiver, in turn, can ac- cept the bid or deny 1.4 Sales marketing interface (T. M. Smith, Gopalakrishna, and Chatterjee 2006) A Three-Stage Model of Integrated Marketing Communications at the MarketingSales Interface The marketing and sales functions in many firms are often at odds despite their common goal of increasing revenue and profit. The finger pointing goes both ways: Marketing complains of poor lead follow-up by sales, and in turn, sales grumbles about the quality of leads generated by marketing. This disconnect can be damaging; high lead volumes generated through effective marketing campaigns could actually hurt downstream sales because of wasted effort on poorly qualified leads and/or delays in sales follow-up resulting from limited sales force capacity. To examine the revenue and profit implications of coordinated communications efforts at the marketingsales interface, the authors develop a three-stage model that captures the effects of sequential marketing/sales communications on lead generation, appointment conversion, and sales closure. The results, which are based on a collaborative effort with a large home improvement retailer, suggest a complex interplay among marketing efforts (multiple media that generate leads), delays in follow-up (time lag between inquiry and sales force contact), and sales efficiencies (appointment and sales conversion). The findings underscore the impact of multimedia spending on the timing and effectiveness of subsequent communications, implying that improved internal collaboration between marketing and sales can offer significant upside potential for the firm. Finally, the authors develop a managerial decision support tool to simulate the impact of varying communications budgets, timing, and allocation on the marketing and sales planning system. 1.5 Firm level impact on salespeople (Salonen et al. 2020) Engaging a product-focused sales force in solution selling: interplay of individual- and organizational-level conditions Solution selling transform from product focused selling . This study explains how manufacturers tackle the critical managerial challenge of transforming a product-focused sales force to undertake solution selling. Through an application of configurational theory, the authors explain how individual and organizational conditions combine to determine salespeoples engagement in solution selling. Multilevel, multisource data from the sales organization of a global supplier of building solutions represent input from salespeople (N = 184), solution champions (N = 23), and sales managers (N = 26). A fuzzy set qualitative comparative analysis reveals no single, optimal way to overcome transformation challenges. Rather, consistent with prior research, solution selling requires certain types of salespeople, because valuebased selling is a necessary condition for successful engagement. Beyond this foundational condition, a heterogeneous sales force can be engaged, as long as the organization provides appropriate support that is tailored to individual salespersons needs. The findings affirm that this viable support can come from either sales managers or solution champions (J. B. Smith and Barclay 1997) The Effects of Organizational Differences and Trust on the Effectiveness of Selling Partner Relationships organizational level selling together Selling alliances that are formed to cooperatively develop and maintain customer relationships are among the new organizational forms that marketing managers utilize for competitive advantage. To be successful, these alliances require sales representatives from allied organizations to work effectively as selling partners. The authors develop a trust-based model of effective selling partner relationships and test it in the context of the computer industry. Par- tial Least Squares analysis of 103 dyadic relationships found that organizational differences were modest predic- tors of three dimensions of mutual perceived trustworthiness, which in turn differentially affected three trusting behaviors. Trusting behaviors were found to have a somewhat greater effect on perceived task performance than on mutual satisfaction, whereas dimensions of trustworthiness had both direct and indirect effects on satisfaction. The authors discuss the managerial and theoretical implications of th 1.6 Salesperson trait orientation (Kidwell et al. 2011) Emotional Intelligence in Marketing Exchanges EQs impact on the sales performance This research examines how sales professionals use emotions in marketing exchanges to facilitate positive outcomes for their firms, themselves, and their customers. The authors conduct three field studies to examine the impact of emotional intelligence (EI) in marketing exchanges on sales performance and customer relationships. They find that EI is positively related to performance of real estate and insurance agents, even when controlling for the effects of domain-general EI, self-report EI, cognitive ability, and several control variables. Sales professionals with higher EI are not only superior revenue generators but also better at retaining customers. In addition, the authors demonstrate that EI interacts with key marketing exchange variablescustomer orientation and manifest influenceto heighten performance such that high-EI salespeople more effectively employ customer-oriented selling and influence customer decisions. Finally, the results indicate a complementary relationship between EI and cognitive ability in that EI positively influences performance at higher levels of cognitive ability. These findings have implications for improving interactions between buyers and sellers and for employee selection and training (McFarland, Rode, and Shervani 2015) Moderator of the EQs effect on job outcomes A contingency model of emotional intelligence in professional selling Abstract Despite significant attention from practitioners and broad claims of the importance of Emotional Intelligence (EI), empirical support for its incremental direct effects on outcomes relevant to professional selling has been disappointing. However, little research has included relevant contextual variables or the potential interactions of EI with contextual variables when considering its effects. This contingency view of EI maintains that EI is important in work settings, but only under certain conditions. Drawing on the appraisal theory of emotions, the authors develop a contingency model, which proposes that salesperson EI moderates the harmful effects of role stress on three work outcomesemotional exhaustion, customer-oriented selling, and sales performance. Using three matched data sources from multiple professional selling workgroups in a business-tobusiness sales setting, the authors find that EI moderates the relationship between role ambiguity and all three outcome variables. (Lam, DeCarlo, and Sharma 2019) Salesperson ambidexterity in customer engagement: do customer base characteristics matter? salesperson orientation moderated by customer base on sales performance This research examines how sales professionals use emotions in marketing exchanges to facilitate positive outcomes for their firms, themselves, and their customers. The authors conduct three field studies to examine the impact of emotional intelligence (EI) in marketing exchanges on sales performance and customer relationships. They find that EI is positively related to performance of real estate and insurance agents, even when controlling for the effects of domain-general EI, self-report EI, cognitive ability, and several control variables. Sales professionals with higher EI are not only superior revenue generators but also better at retaining customers. In addition, the authors demonstrate that EI interacts with key marketing exchange variablescustomer orientation and manifest influenceto heighten performance such that high-EI salespeople more effectively employ customer-oriented selling and influence customer decisions. Finally, the results indicate a complementary relationship between EI and cognitive ability in that EI positively influences performance at higher levels of cognitive ability. These findings have implications for improving interactions between buyers and sellers and for employee selection and training 1.7 Salesperson non-sales activity/ service (Johnson and Sohi 2015) Understanding and resolving major contractual breaches in buyerseller relationships: a grounded theory approach Abstract In business-to-business relationships, sellers are often faced with instances of contractual breaches by buyers. In many cases, relationship factors preclude legal enforcement of contract terms, requiring sellers to explore alternate resolution options. Literature on contractual breaches has primarily focused on enforcement options based on terms specified in the contract. However, little is known about how companies deal with contractual breaches by their customers when legal enforcement is not a viable option. The authors use a grounded theory approach to investigate this important issue. Based on in-depth interviews with 40 supplier managers and executives in multiple industries, the authors identify: (a) types of out-ofcontract alternatives for resolving breaches, (b) factors that lead to use of enforcement options outside the terms specified in the contract, (c) contextual influences, and (d) individual and firm-level consequences of outside-ofcontract enforcement. (Malshe and Friend 2018) Initiating value co-creation: Dealing with non-receptive customers deal with co-creation reluctance. co-creation to improve the offering. Scholarly emphasis on the significance of integrating customer and supplier work processes to co-create customer value is increasingly important. While suppliers and customers working closely is imperative for the success of the value co-creation (VCC) process, customers reluctance at times to allow suppliers into their environments has not been fully explored. The present study is an expansive qualitative inquiry consisting of 114 in-depth interviews across 57 business-to-business evaluations that aims to understand both the nuanced nature of customer non-receptivity to specific VCC initiatives and those strategies suppliers may adopt to successfully manage it. Findings elicit three potential kinds of customer non-receptivity: apathy, ambivalence, and annoyance. Furthermore, conclusions propose six strategies suppliers may use to manage customer non-receptivity: intrinsic initiative, inspiration and implementation, complexity absorption, value alignment, credibility building, and objective centrality. (Panagopoulos, Rapp, and Ogilvie 2017) Salesperson Solution Involvement and Sales Performance: The Contingent Role of Supplier Firm and CustomerSupplier Relationship Characteristics salespeople invovled in the solution (not solution selling)s impaact on the long term performance Salespeople play a crucial role in their firms efforts to provide customer solutions. However, little research has examined how salesperson involvement in customer solutions can be conceptualized, whether it pays off, and what boundary conditions might heighten its performance effects. This study addresses these gaps and offers a conceptualization of salesperson solution involvement by focusing on the set of salesperson-related activities that enact the four relational processes inherent in customer solutions. The authors collect a unique data set that includes a wide range of firms, industries, and countries, as well as the perspectives of both salespeople and customers, across five studies. Results validate the stability of the conceptualization across contexts. They also reveal that salesperson solution involvement is systematically related to increases in both subjective and objective, time-lagged measures of sales performance. Finally, results show that the performance effects of salesperson solution involvement are amplified under higher levels of firms product portfolio scope, sales unit cross-functional cooperation, and customersupplier relationship tie strength. Surprisingly, customer adaptiveness is not found to moderate the performance effects of salesperson solution involvement. 1.8 B2G selling (Chase and Murtha 2019) In business-to-government and business-to-business transactions, suppliers often have limited access to buyers during the buying process. The authors term these buyers barricaded buyers. Despite the prevalence of barricaded buyers in practice, research has remained largely silent on the topic. Therefore, the authors combine insights from eight organizational purchasing case studies and individual interviews with signaling theory to advance a conceptual framework that highlights ways a supplier can increase its competitiveness (and, correspondingly, its selection likelihood) when selling to barricaded buyers. The framework reflects three distinct ways in which signaling occurs or influences the barricaded buying process: the seller signals to buyers (e.g., through novel solutions, explicit responding), the seller signals to competing sellers (e.g., through peacocking), and the buyer signals to sellers whose meaning is jammed (e.g., through supplier-specific capabilities and language). The framework invokes barricade restrictiveness as an important contingency variable that lends nuance to when the signaling activities are most likely to affect suppliers competitiveness. References "],["literature-review-note.html", "Chapter 2 Literature Review Note 2.1 Persuasion knowledge Model Review", " Chapter 2 Literature Review Note 2.1 Persuasion knowledge Model Review 2.1.0.1 Theoretical development (Campbell and Kirmani 2000) RQ &amp; Concept: When will the consumer use the persuasion knowledge? Persuasion knowledge refers to consumers theories about persuasion and includes beliefs about marketers motives, strategies, and tactics; What kind of persuasion tactics are effectiveness and appropriateness; psychological mediators of tactic effectiveness (i.e., if the influencer is trying to gain trust, to invoke sympathy); and ways of coping with persuasion attempts. Theory: Persuasion knowledge is an effortful elaboration. Thus, the consumer needs both the cognitive bandwidth to think and the accessibility to the cue that the agent has the ulterior motive. Finding: When an ulterior persuasion motive is highly accessible, both cognitively busy targets and unbusy observers use persuasion knowledge to evaluate the salesperson. When an ulterior motive is less accessible, cognitively busy targets are less likely to use persuasion knowledge, evaluating the salesperson as more sincere than are cognitively unbusy observers. Contribution: a) The first paper empirically investigates when the persuasion knowledge will be used. b) The outcome is the perception of the salespeople (i.e., sincerity). c) minor contribution: the method to manipulate accessibility and cognitive busyness (e..g, priming, target observer status) Critique: a) The observer doesnt care about the purchase. Thus, s/he focuses too much on over-interpreting the salespeoples persuasion attempt. For example, when the salespeople have absolutely no intention to persuade (e.g., after the purchase is made), the observer still suspects the salespeople much more than the actor. However, the observers use of persuasion knowledge may be of little interest in strategy research. I am not sure about the usefulness of the finding to the manager. 2.1.0.2 Marketing strategy application (Thompson and Malaviya 2013): RQ &amp; Concept: How does awareness of advertising co-creators identity help or hurt persuasion perceived by the audience? consumer-generated ads: the consumer participated in the ads creation process. Theory: An important element of consumer persuasion knowledge is their belief about the persuasion competence of agents or to what extent the agents are perceived to know about effectively influencing buying decisions. When the ads creator is a consumer, the consumer is perceived to be less competent (i.e., do not know how to create a professional ad). The less competent creator leads the audience to perceive the ads as less persuasive. As a consequence, the evaluation of the ads is lower. Finding: The authors find that disclosing ads is created by a consumer will lead to negative advertisement and brand evaluations. The effect is mediated through skepticism path (-) and identification path (+). The two paths are moderated by cognitive resources and background information about the ad creator and brand loyalty. Contribution: The papers WoW factor is a) it is hard to guess which path would dominate skepticism or identification. The intuition is the identification, but the result is skepticism. b) prior research only looks at the positive effect of consumer-generated content. However, in this case, it is negative. 2.1.1 Summary for PKM in online environment Overall, the consumer is more likely to use the persuasion knowledge if they can access the ulterior motivation as in the context of the advertising (vs. consumer report) and the salespeople negotiation. The online environment should make the accessibility even higher as the consumer tends to be more skeptical of the salespeople when they cannot observe the non-verbal cues. The online environment can be ambiguous if the consumer has cognitive bandwidth. On the one hand, the consumer doesnt need to pay attention to maintaining the non-verbal cues, which can be resources draining. On the other hand, the consumer may not have the full attention to the interaction as the many background tasks distract the consumer. Therefore, it is hard to predict if the online environment will be more conducive to using user-generated ads. If the bandwidth is low, then the user-generated ads will be more effective. However, when the bandwidth is high or the consumer is involved in processing the information (i.e., they click the ads), then the user-generated ads will be less effective. When the consumer can access the persuasion knowledge, it seems that the result is often negative compared to when they cannot access it. For example, the foot-in-the-door tactic would be less effective in getting into the door if the consumer senses the salespeople are tricking him/her. However, comparing persuasion vs. no persuasion, the effect on the subjects influence is less clear. As the persuasion attempt becomes less effective when PKM is invoked, yet no persuasion is ineffective in driving the action. For the online environment, if the salespeople persuade using scarcity or non-scarcity tactics, the former is more likely to raise skepticism, yet it is also more effective in driving action in general. Based on the PKM, the persuasion without scarcity tactic that could raise skepticism would be more effective. References "],["database-marketing-substantive-domain.html", "Chapter 3 Database marketing substantive domain 3.1 Key Issues 3.2 Method 3.3 Sub - substantive areas", " Chapter 3 Database marketing substantive domain 3.1 Key Issues 3.1.1 Data Privacy 3.1.2 Customer lifetime value (LTV) 3.2 Method 3.2.1 RFM 3.2.2 Market basket analysis 3.2.3 Collaborative filtering 3.2.4 Cluster analysis 3.2.5 Decision trees 3.2.6 Machine learning 3.3 Sub - substantive areas 3.3.1 Acquisition 3.3.2 Retention/ Churn management 3.3.3 Cross-selling and up-selling 3.3.4 Reward program 3.3.5 Multichannel customer "],["data-scientist-job.html", "Chapter 4 Data Scientist Job 4.1 Business strategy Track (a.k.a Marketing Analytics) 4.2 Consumer insight Track (aka. Marketing Research) 4.3 Optimization Track (a.k.a Operational Research)", " Chapter 4 Data Scientist Job In general, the data scientist role can be divided into decision scientist and the machine scientist The Kinds of Data Scientist (hbr.org) 4 Types of Data Science Jobs | Udacity The entry-level (i.e., data analyst) deals with basic analysis tools such as Excel and SQL programming skills to pull data. The middle-level (i.e., data scientist I) deals with more advanced analytics tools such as R, Python. The senior-level uses the same analytics tool but can write or modify the published package/ library. On a side note, the data product management side deals with data product/service, which is akin to the product manager in general. In practice, the substantive job content can be differentiated based on strategy, consumer behavior, and optimization tracks. The strategy track produces the analysis for managers to make further decisions. In the consumer behavior track, they produce the analysis to elucidate the psychological mechanism and come up with interesting mental models used by the consumers. The optimization track focuses on making things more efficient on a large scale or using machine learning to automate the analysis. I list the specific duties of each track below, along with the resources to develop the corresponding skills or business sense. 4.1 Business strategy Track (a.k.a Marketing Analytics) 4.1.1 Database marketing RFM targeting offer design, discount offer optimization See the Database marketing page 4.1.2 Programming SQL: Data quality validation Data manipulation and merging https://mailmissouri-my.sharepoint.com/:b:/g/personal/ylb3c_umsystem_edu/ETfI6jNCUhhJvMF-edkqqZEBu5gcbJ0CD92edzkvNjPjwQ?e=u3MJid Python https://www.notion.so/Python-practice-110-2a8c3c764f6a489b911c8e1c432ea165 Introduction to Data Science in Python - Week 1 | Coursera map() lambda list comprehension numpy library series, data frame groupby, pivot, merging Distribution R Base MAP  same function to all variables. Ggplot  viusalization Tidyr  data manipulation &lt;https://mailmissouri-my.sharepoint.com/:f:/g/personal/ylb3c_umsystem_edu/Em8gdLAUHelNud9b7wGlTr8B6L5tVHwYiVyyh6t112Gvhg?e=i2r0PZ&gt; basic commands model accuracy KNN Classification - LDA? Bootstraping Regularization Non-linear model Tree based SVM Unsupervised 4.1.3 Statistics: Statistical tests for differences: Independent, Paired T tests, F- tests, Chi-square tests (used for A/B testing, incrementality testing) Incrementality Regression Independent variables: dummy variable, variable transformation, exploratory/ descriptive analysis Dependent variable: Binary (logit, probit regression), Count (poission, negative binomial regression), Censored (Tobit, survival regression) Advanced: Causal inference Control for observables Mixed model / Hierachical linear model Difference in Difference Regression Discontinuity Modeling process 4.1.4 Visualization Drawing Interaction plot Decile plots Written report on the analysis 4.1.5 Automation Function building Analysis library 4.2 Consumer insight Track (aka. Marketing Research) Qualitative study design Quantitative study design (i.e., survey) Qualtrics for questionnaire or experiment design Generating research questions Data analysis Meditation , Path Model Measurement model 4.3 Optimization Track (a.k.a Operational Research) 4.3.1 Model optimization Machine learning - regularization Machine Learning (coggle.it) dimension reduction Chapter 34 Model Stacking | Advanced Data Analysis (bookdown.org) 4.3.2 Macro level models Marketing Mix Models https://www.notion.so/A-Better-Way-to-Calculate-the-ROI-of-Your-Marketing-Investment-f0b034ef75154ffea26bf81b3fda2b2c Ad stock function Media Attribution Modeling Attribution Modeling in Marketing Python | Kaggle https://mailmissouri-my.sharepoint.com/:b:/g/personal/ylb3c_umsystem_edu/EfV-Ld_JH8FNiCqp6MtSkFMBAcWzyMHvVKmBhdCyLT7k2A?e=TtLaQ1 4.3.3 Academic Paper implementation Conceptual understanding Composing or Modifying R, Python package per the papers idea https://mailmissouri-my.sharepoint.com/:f:/g/personal/ylb3c_umsystem_edu/EkeM-q-_RDRHg5djz2H6kwYBRqV__im7kNP7NhJP4nKj1w?e=sb0t5c "],["marketing-strategy-phd-skills.html", "Chapter 5 Marketing Strategy PhD skills 5.1 Reading 5.2 Writing", " Chapter 5 Marketing Strategy PhD skills Whats unique about my approach? Mixing the oriental and western PhD training. Chinese scholars are better at summarizing the teaching/ standardizing American scholars are better at originality/ high quality questions 5.1 Reading 5.1.1 Meta skill to gain Screen information quickly in this information overload age Figure content with enough depth by discussing with other people/ expert screening Acquire the beginner to expert mindset, so that you can be the teacher to give feedback to students 5.1.2 Reading Purpose 5.1.2.1 Conversant reading/ narrow build a system of knowledge mgmt - in R book or Notion (30 mins summary) Extract word, logic and structure of the papers in your field (90-120 mins read) - variables, measurement, method, research design, theory Practice screening information to see the big picture (10 mins skim) How to train: detailed note. 2 pgs on one paper + challenger reading + response paper on several key conversant Two types of papers: 1) Theory paper a) gist of thesis b) logic of the argument c) propositions Empirical paper gist of RQ b) logic of argument c) hypothesis - key finding d) Method, measure 5.1.2.2 Get to know a field/ broad How to train: abstract summary. 1 pgs on multiple papers. 5.1.2.3 Method/ sectioning 5.1.2.4 Hobby/ creative 5.1.3 How to train 5.1.4 Suggested reading (optional) 5.2 Writing "],["applied-stats-model-ii---exam-1.html", "Chapter 6 Applied Stats Model II - Exam 1 6.1 1. (7 points) In a poisson regression problem with one explanatory variable x, the estimate of the  6.2 2. (5 points) In a generalized linear model, why is the null deviance typically larger than the residual deviance? 6.3 3. (5 points) Can standard linear regression ever be used when the data are counts? Explain why or why not. 6.4 4. Take the following random effect model 6.5 5. Utilizing the below code and output, answer the following: 6.6 6. Take the data given by Problem6Data.csv, which contains a continuous response y and two predictors x1 and x2. 6.7 7. The ships dataset in the MASS package provides the number of incidents(indicents) resulting in ship damage as a function of total months of service(service), ship type(type), as well as:  Year of ship construction(year)  Broken into 5 year increments with the years variable value indicating the beginning of this period. e.g. year=60 refers to 1960-1964  Period of operation(period) with realizations:  60: 1960 - 1974  75: 1975 - 1979 The data can be loaded with: data(ships, package=MASS)", " Chapter 6 Applied Stats Model II - Exam 1 6.1 1. (7 points) In a poisson regression problem with one explanatory variable x, the estimate of the  coefficient under a log link is  = 0.5. If, when x = 1 we can obtain a predicted count of µ = 4. What is the predicted count when x = 3? # since the link is log, I transform it back to the count or 1.65 exp(0.5) ## [1] 1.648721 #the intercept is 0.8862944 = log(4)-1*0.5 #??? does the intercept needs the log? -&gt; I assume mu is after the exp().Thus, I take a log(). log(4)- 1*0.5 ## [1] 0.8862944 #when x = 3, 3*beta = 4.95 3 * 1.65 ## [1] 4.95 #Thus, the predicted count is exp(0.8862944) + 3*1.65 ## [1] 7.376123 The predicted count when the x =3 is 7.37 or 7 times. 6.2 2. (5 points) In a generalized linear model, why is the null deviance typically larger than the residual deviance? The null model only contains the intercept, while the residual deviance comes from the model with other predictors. If the other predictors are useful in the prediction, the deviance will be smaller. 6.3 3. (5 points) Can standard linear regression ever be used when the data are counts? Explain why or why not. Yes it can run and predict the counts as the count can be expressed linearly. However, the result wont be valid. First, the count data are strickly 0 or positive. linear regression will lead to negative prediction. Second, The distribution of error terms wont be normal as there is often 0 inflation in the count data. 6.4 4. Take the following random effect model yij = µ + i + ij . This model was fit using lmer() and analyzed with the below. TheModel &lt;- lmer(y ~ (1 | alpha), data= TheData) summary(TheModel) ## Linear mixed model fit by REML \\[&#39;lmerMod&#39;\\] ## Formula: y ~ (1 | alpha) ## Data: TheData ## ## REML criterion at convergence: 5035925 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -5.0460 -0.6744 -0.0009 0.6759 4.6942 ## ## Random effects: ## Groups Name Variance Std.Dev. ## alpha (Intercept) 2.435 1.561 ## Residual 9.007 3.001 ## Number of obs: 1000000, groups: alpha, 10 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 14.9258 0.4935 30.25 ranef(TheModel)$alpha ## (Intercept) ## 1 1.8086783 ## 2 -0.5872736 ## 3 2.2398284 ## 4 -1.5152633 ## 5 0.1962228 ## 6 -2.0713461 ## 7 0.5250623 ## 8 -2.0133227 ## 9 -0.1044895 ## 10 1.5218990 6.4.1 Complete the following: 6.4.2 (3 points) Find the intra class correlation coefficient. #S7 MM-RandomEffects_Spring2022 # ICC is the alpha variance out of total variance ICC = 2.435 / (2.435 + 9.007) ICC ## [1] 0.2128124 The intra class correlation is 0.21 6.4.3 (2 points) Which level of the random effect  will have the largest predicted value? at level 3, as the coefficient is 2.23 6.4.4 (2 points) Predict y for level 6 of the random effect. #S23 MM-RandomEffects_Spring2022 # fixed effect intercept + random effect at level 6 14.9258 + -2.0713461 ## [1] 12.85445 The predicted y will be 12.85 6.5 5. Utilizing the below code and output, answer the following: 6.5.1  (3 points) Why are the residual and null deviance values the same? They are the same since the model only contains the intercept which essentially is the null model. 6.5.2  (2 points) Based on these deviance values, does the model fit well? Does this make sense? It doesnt fit well as the variance is as large as the null model. It make sense, since they are the same model. SomeData &lt;- rgamma(n = 30, shape = 1.5, scale = 1.5) AModel &lt;- glm( SomeData ~ 1, family = Gamma ) summary(AModel) ## ## Call: ## glm(formula = SomeData ~ 1, family = Gamma) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6595 -0.7681 -0.2296 0.2560 1.5974 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.44293 0.06558 6.754 2.06e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.6576803) ## ## Null deviance: 20.077 on 29 degrees of freedom ## Residual deviance: 20.077 on 29 degrees of freedom ## AIC: 109.17 ## ## Number of Fisher Scoring iterations: 6 6.6 6. Take the data given by Problem6Data.csv, which contains a continuous response y and two predictors x1 and x2. P6 &lt;- read.csv(&quot;E:/Cloud/OneDrive - University of Missouri/Mizzou_PhD/Class plan/Applied Stats Model II/Exam1/Problem6Data.csv&quot;, stringsAsFactors = TRUE) 6.6.1 a. (5 points) Read the data into R and make some plots between the response and predictors. Describe the patterns and comment on behaviors you observe with exact 0s. plot(y ~ x1,P6) plot(y ~ x2,P6) library(tidyverse) # for mutate function ## -- Attaching packages --------------------------------------- tidyverse 1.3.1 -- ## v ggplot2 3.3.5 v purrr 0.3.4 ## v tibble 3.1.6 v dplyr 1.0.8 ## v tidyr 1.2.0 v stringr 1.4.0 ## v readr 2.1.2 v forcats 0.5.1 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Plot_DF &lt;- P6 |&gt; group_by(y) |&gt; summarise(count=n()) |&gt; #n() is the summarise function option mutate(etotal=sum(count), proportion=count/etotal) Plot_DF ## # A tibble: 451 x 4 ## y count etotal proportion ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 50 500 0.1 ## 2 1.00 1 500 0.002 ## 3 1.00 1 500 0.002 ## 4 1.00 1 500 0.002 ## 5 1.00 1 500 0.002 ## 6 1.01 1 500 0.002 ## 7 1.02 1 500 0.002 ## 8 1.05 1 500 0.002 ## 9 1.05 1 500 0.002 ## 10 1.06 1 500 0.002 ## # ... with 441 more rows The y and x1 and x2 have a positive relationship. as x1 or x2 increases the y also increases. There are 10% of y are 0s. 6.6.2 b. (5 points) Make a histogram of the response variable and comment on the shape. Are the values strictly positive? hist(P6$y) The response variable is highly right skewed with many 0s. The values are 0 and positive. 6.6.3 c. (4 points) State the range that the index parameter p, for use in the Tweedie distribution, must be contained in. Why is this the case? The range should be seq(from=1.2,to=1.8,by=0.1) since there are 0s and positive values. 6.6.4 d. (6 points) Find an optimal value of p for use in the Tweedie Distribution, using a log link. # why sometimes work, sometimes doesn&#39;t??? library(tweedie) TW_p_log &lt;- tweedie.profile(y ~ ., p.vec = seq(from=1.2,to=1.8,by=0.05), link.power = 0, do.plot = TRUE,# log link data=P6) ## 1.2 1.25 1.3 1.35 1.4 1.45 1.5 1.55 1.6 1.65 1.7 1.75 1.8 ## .............Done. Tweedie_p_log = TW_p_log$p.max Tweedie_p_log ## [1] 1.322449 The optimal value is 1.32 6.6.5 e. (5 points) Again using a log link, fit a Tweedie Distribution using the index parameter value you obtained in part d. library(statmod) # For tweedie family in glm() clot_Tweedie_log &lt;- glm(y ~ ., data=P6, family=tweedie(var.power=TW_p_log$p.max,link.power=0)) summary(clot_Tweedie_log) ## ## Call: ## glm(formula = y ~ ., family = tweedie(var.power = TW_p_log$p.max, ## link.power = 0), data = P6) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -6.6608 -1.7594 -0.2129 1.0988 6.1159 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.129872 0.079286 14.25 &lt;2e-16 *** ## x1 0.159589 0.005697 28.01 &lt;2e-16 *** ## x2 0.163736 0.005503 29.76 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Tweedie family taken to be 3.76473) ## ## Null deviance: 8883.3 on 499 degrees of freedom ## Residual deviance: 2433.2 on 497 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 6 6.6.6 f. (6 points) For each of the values of x2 given by the R vector x2_vals &lt;- seq(0, 15, by= .01), use the compound Poisson-Gamma structure to predict the probability of obtaining an exact 0 response when x1 is at its mean value. summary(P6) ## y x1 x2 ## Min. : 0.000 Min. : 0.02041 Min. : 0.005161 ## 1st Qu.: 9.546 1st Qu.: 3.46072 1st Qu.: 3.388906 ## Median : 38.008 Median : 7.48721 Median : 7.396909 ## Mean : 56.206 Mean : 7.50728 Mean : 7.408334 ## 3rd Qu.: 86.223 3rd Qu.:11.35581 3rd Qu.:11.334116 ## Max. :304.515 Max. :14.99988 Max. :14.979441 # Generate the new dataset. # create a new dataset with x2 taking all value, x1 taking mean. Then the count using the new data. data_x2 &lt;- data.frame(x2 = seq(0, 15, by= .01)) data_x1 &lt;- data.frame(x1 = runif(n = 1501, min = 7.50728, max = 7.50728)) data_gen = cbind(data_x1,data_x2) mu.data_gen &lt;- predict(clot_Tweedie_log, newdata=data_gen,type=&quot;response&quot;) p_MLE &lt;- TW_p_log$p.max phi_MLE &lt;- TW_p_log$phi.max #mu.data_gen Prob_Zero_Model &lt;- exp(-mu.data_gen^(2 - p_MLE)/(phi_MLE*(2 - p_MLE))) summary(Prob_Zero_Model) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0001304 0.0027375 0.0203945 0.0452845 0.0767088 0.1838113 The probability to attain 0 response when x1 is at its mean is 4.5%. # produce the actual 0 vs. predicted 0 comparison #library(tidyverse) #quilpie |&gt; group_by(Phase) |&gt; summarize(prop0_dat = mean(Rain==0)) |&gt; cbind(Prob_Zero_Model) #??? why is the actual data 10% 0, but here is 4.5%? Diff so much 6.7 7. The ships dataset in the MASS package provides the number of incidents(indicents) resulting in ship damage as a function of total months of service(service), ship type(type), as well as:  Year of ship construction(year)  Broken into 5 year increments with the years variable value indicating the beginning of this period. e.g. year=60 refers to 1960-1964  Period of operation(period) with realizations:  60: 1960 - 1974  75: 1975 - 1979 The data can be loaded with: data(ships, package=MASS) data(ships, package=&quot;MASS&quot;) 6.7.1 a. (4 points) Use group_by and summarise to compute the average months of service for each combination of year of construction and period of operation. Explain why there is a 0 in the result. #ships$year &lt;- as.factor(ships$year) #ships$period &lt;- as.factor(ships$period) library(tidyverse) # for mutate function Plot_DF_Service &lt;- ships |&gt; group_by(year, period) |&gt; summarise(mean=mean(service)) #variable name is mean ## `summarise()` has grouped output by &#39;year&#39;. You can override using the ## `.groups` argument. #Plot_DF_Service #??? https://www.rdocumentation.org/packages/dplyr/versions/0.7.8/topics/summarise # Why does it need mean = mean()? duplication? Plot_DF_Service ## # A tibble: 8 x 3 ## # Groups: year [4] ## year period mean ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 60 60 9297. ## 2 60 75 3579. ## 3 65 60 6312. ## 4 65 75 4554 ## 5 70 60 2173 ## 6 70 75 4354. ## 7 75 60 0 ## 8 75 75 2446. # then why the 70 and 60 combination has service??? The 0 appears when the construction year is 75 (1975-1979) and the period of operation is 60 (1960-1964). The ships construction is after its period of operation, thus no service is recorded. 6.7.2 b. (3 points) Create a new filtered dataset by removing the rows where service=0. library(dplyr) #must run the library again to avoid the duplicated names # filter the data ships_filter = filter(ships, ships$service &gt; 0) 6.7.3 c. (10 points) Fit both a Poisson rate model and Negative Binomial rate model model using number of incidents per service month as the response and all other variables as predictors. Compare which model is better using AIC. # is it the aggregated model vs. individual count? or the same??? # using the filtered data? ships_filter$incidents_month = ships_filter$incidents / ships_filter$service Poisson_Mod &lt;- glm(incidents ~ year + period + type + service, family=poisson, ships_filter) summary(Poisson_Mod) ## ## Call: ## glm(formula = incidents ~ year + period + type + service, family = poisson, ## data = ships_filter) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.4598 -1.6558 -0.3323 0.6261 3.5104 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.387e+00 1.151e+00 -3.811 0.000138 *** ## year 5.280e-02 1.378e-02 3.831 0.000127 *** ## period 3.642e-02 9.245e-03 3.939 8.19e-05 *** ## typeB 9.622e-01 2.058e-01 4.675 2.94e-06 *** ## typeC -1.212e+00 3.274e-01 -3.701 0.000215 *** ## typeD -8.652e-01 2.875e-01 -3.009 0.002619 ** ## typeE -1.105e-01 2.350e-01 -0.470 0.638160 ## service 4.785e-05 7.050e-06 6.787 1.15e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 614.54 on 33 degrees of freedom ## Residual deviance: 120.56 on 26 degrees of freedom ## AIC: 234.43 ## ## Number of Fisher Scoring iterations: 5 NegBinom_Mod &lt;- MASS::glm.nb(incidents ~ year + period + type + service, ships_filter) # Convert back to glm() style object ??? why needs a conversion? # why alternative style reached? NegBinom_Mod_glm &lt;- MASS::glm.convert(NegBinom_Mod) summary(NegBinom_Mod_glm, dispersion=1) ## ## Call: ## stats::glm(formula = incidents ~ year + period + type + service, ## data = ships_filter, family = negative.binomial(theta = 2.4737185912729, ## link = log)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4048 -1.2366 -0.2498 0.3623 1.9697 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.461e+00 2.322e+00 -3.644 0.000269 *** ## year 1.154e-01 3.147e-02 3.667 0.000245 *** ## period 3.068e-02 2.075e-02 1.478 0.139299 ## typeB 7.485e-01 5.544e-01 1.350 0.176967 ## typeC -9.342e-01 4.910e-01 -1.903 0.057077 . ## typeD -8.513e-01 4.860e-01 -1.752 0.079843 . ## typeE 1.650e-01 4.458e-01 0.370 0.711302 ## service 7.692e-05 2.186e-05 3.518 0.000434 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(2.4737) family taken to be 1) ## ## Null deviance: 141.113 on 33 degrees of freedom ## Residual deviance: 39.249 on 26 degrees of freedom ## AIC: 193.89 ## ## Number of Fisher Scoring iterations: 1 6.7.4 d. (7 points) Using the better model selected in part c, assess the significance of each term using drop1(). Which terms are insignificant? Remove the insignificant variables from the model in c, and refit. #Neg binomial is better. drop1(NegBinom_Mod,test=&quot;F&quot;) ## Single term deletions ## ## Model: ## incidents ~ year + period + type + service ## Df Deviance AIC F value Pr(&gt;F) ## &lt;none&gt; 39.249 191.89 ## year 1 50.604 201.25 7.5221 0.01089 * ## period 1 41.553 192.19 1.5267 0.22765 ## type 4 50.667 195.31 1.8910 0.14211 ## service 1 48.606 199.25 6.1987 0.01951 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 All the terms are significant at 95% confidence level. 6.7.5 e. (6 points) Statistically compare the residual deviance in your final model from d to the null deviance. pchisq(summary(NegBinom_Mod)$deviance,NegBinom_Mod$df.residual, lower.tail = FALSE) ## [1] 0.046152 The difference is significant at 95 % confidence level. Thus, the model in d is significantly better than the null model. 6.7.6 f. (4 points) Compare the AIC from the reduced model in d to the corresponding value from c. Is the change you see expected? Also examine the residual deviance of the model in d, what does it say about the models fit to the data? #??? all are significant no change? Reduced models lowest AIC is 181 vs. the model in c is 183. The change is very small since the reduced model essentially is the same as the c (i.e., all variables are significant predictors). Residual variance in d 42 or the same as the c. The model fits the data well. 6.7.7 g. (6 points) Make a plot of the deviance residuals vs the predicted values(link scale). How is the fit? library(DALEX) ## Welcome to DALEX (version: 2.4.0). ## Find examples and detailed introduction at: http://ema.drwhy.ai/ ## Additional features will be available after installation of: ggpubr. ## Use &#39;install_dependencies()&#39; to get all suggested dependencies ## ## Attaching package: &#39;DALEX&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## explain library(auditor) ## ## Attaching package: &#39;auditor&#39; ## The following object is masked from &#39;package:DALEX&#39;: ## ## model_performance i_NegBiExp &lt;- DALEX::explain(NegBinom_Mod, data = ships_filter, y = ships_filter$incidents,verbose = FALSE) i_NegBiHalfNorm &lt;- model_halfnormal(i_NegBiExp) ## Negative binomial model (using MASS package) #S16 plot_halfnormal(i_NegBiHalfNorm, quantiles = TRUE) #S30 #Error: &#39;model_halfnormal&#39; is not an exported object from &#39;namespace:DALEX&#39; -&gt; need auditor library. The fit is good. There is no over-dispersion issue anymore. "],["question-1.html", "Chapter 7 Question 1: 7.1 a. (8 points) Create plots to examine how launch speed and angle may affect the probability of a home run and describe your findings. 7.2 b. (9 points) Fit a logistic regression model with home_run as the response and all other variables as predictors. Conduct a deviance test to assess if this model is better than the null model. 7.3 c. (8 points) Conduct deviance tests with the drop1() function to assess the significance of each individual variable and report the results. Compare the p-values to those obtained from summary(). 7.4 d. (6 points) Fit a smaller model after removing all variables which are insignificant using  = 0.05. Compare this model to the larger model, are they significantly different? What are the implications of this with regard to model selection? Until the end of this question, use the smaller model for all analysis 7.5 e. (7 points) How does the launch speed after the ball is hit affect the odds of HomeRun occurring? Provide a confidence interval for this value. 7.6 f. (5 points) Using the deviance residuals, make a binned residual vs fitted probability plot and comment on the fit of the model. 7.7 g. (4 points) Using a probability of 0.5 as the threshold for predicting an observation yielding a home run, create a table classifying the predictions against the observed values. Describe your findings. What is the misclassification rate? 7.8 h. (6 points) Using probability thresholds from 0.005 to 0.995, obtain the sensitivities and specificity of the resulting predictions. Create an ROC plot and comment on the effectiveness of the models ability to correctly classify the response. As we vary the threshold to determine classifications, is inverse relationship between sensitivity and specificity strongly evident? 7.9 i. (5 points) Produce a plot of the sensitivity and specificity against the threshold. Is there a threshold for classification you would recommend that provides a good balance between the two? Make another confusion matrix using this cutoff, how does the result compare to the previous one? Consider the types of errors you observe. 7.10 j. (3 points) Consider a logistic model with only launch_angle and launch_speed being used to predict the probability of a home run. What is the AIC of this model? 7.11 k. (11 points) Create a dummy variable which is 1 if launch_angle is between 20 and 40 degrees and use this variable in your model instead of the raw value for launch_angle. Then, complete the following: 1. Compare the AIC of this model to the model in part j, which model is better? 2. What does the coefficient of your dummy variable mean? Interpret the value. 3. Interpret the value of the intercept by converting to a probability. Does this result make sense?", " Chapter 7 Question 1: \\(1\\) Logistic Regression: Major League Baseball (MLB) maintains a public facing version of its Statcast system. Since 2015, MLB teams have been leveraging data analytics to gain a competitive advantage using the raw data out of Statcast, which captures extremely high resolution views of player performance. Here we will study a dataset I have pulled and curated. We will examine which of several characteristics are predictive of a hit yielding a home run. The selected variables are as follows:  pitch_speed: Speed of pitch in mph.  pitch_spin: Spin rate of pitch  pitch_horizontal_movement: Horizontal movement of the pitch in feet from the catchers perspective.  pitch_vertical_movement: Vertical movement of the pitch in feet from the catchers perpsective.  plate_x: Horizontal position of the ball when it crosses home plate from the catchers perspective.  plate_z: Vertical position of the ball when it crosses home plate from the catchers perspective.  pitcher_hand: Pitchers throwing hand.  launch_angle: Launch angle of the batted ball.  launch_speed: Exit velocity of the batted ball (so, how fast it is going once hit by the batter)  stand: Side of the plate batter is standing from the pitchers perspective.  home_run: 1/0, the target variable The dataset can be loaded with the command HomeRun &lt;- read.csv(/HomeRunData.csv, stringsAsFactors = TRUE) #??? Cannot knit the single document? # Input the dataset HomeRun &lt;- read.csv(&quot;E:/Cloud/OneDrive - University of Missouri/Mizzou_PhD/Class plan/Applied Stats Model II/HW1/HomeRunData.csv&quot;, stringsAsFactors = TRUE) 7.1 a. (8 points) Create plots to examine how launch speed and angle may affect the probability of a home run and describe your findings. On the first plot between launch speed and the probability of a home run, as the launch speed increase to 80 and above, the home run chance increases. On the second plot between launch angle and the probability of a home run, as the launch angle increase to be above 10, the chance of home run increases. However, when it surpass 50 the chance of home run decreases. # S39. class is the DV, thickness is the IV. ??? why it doesn&#39;t need binning but deviances needs it? Why is my output different from the ppt? plot(home_run ~ launch_speed,HomeRun) # as the lauch speed increase to 80, the home run chance increase. plot(home_run ~ launch_angle,HomeRun) # seems to have an u-shape effect. less than 0 and more than 50 are both bad. 7.2 b. (9 points) Fit a logistic regression model with home_run as the response and all other variables as predictors. Conduct a deviance test to assess if this model is better than the null model. I ran the logistic regression to predict home run using all other variables. The results show the pitch_speed, launch angle and launch speed are statistically significant predictors at 95% confidence level. I then conducted the deviance test. The result is significant. Thus, the model is better than the null model. # Fit model with all variables. Note &lt;- not = Logistic_Model &lt;- glm(home_run ~ ., family = binomial, data = HomeRun) # Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred. ??? What does the warning mean? still can run. move on for now. # https://www.statology.org/glm-fit-fitted-probabilities-numerically-0-or-1-occurred/ summary(Logistic_Model) ## ## Call: ## glm(formula = home_run ~ ., family = binomial, data = HomeRun) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.7030 -0.1785 -0.0455 -0.0036 4.2501 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.288e+01 1.073e+00 -30.632 &lt; 2e-16 *** ## pitch_speed -5.416e-02 8.723e-03 -6.209 5.32e-10 *** ## pitch_spin 6.729e-05 1.221e-04 0.551 0.581 ## pitch_horizontal_movement -2.779e-02 5.151e-02 -0.539 0.590 ## pitch_vertical_movement -5.401e-02 7.207e-02 -0.749 0.454 ## plate_x -4.299e-02 8.223e-02 -0.523 0.601 ## plate_z 1.040e-01 7.133e-02 1.458 0.145 ## pitcher_handR -6.163e-02 9.391e-02 -0.656 0.512 ## launch_angle 9.850e-02 2.962e-03 33.253 &lt; 2e-16 *** ## launch_speed 3.250e-01 7.796e-03 41.691 &lt; 2e-16 *** ## standR -8.384e-02 7.141e-02 -1.174 0.240 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 10624.6 on 29595 degrees of freedom ## Residual deviance: 6109.7 on 29585 degrees of freedom ## AIC: 6131.7 ## ## Number of Fisher Scoring iterations: 9 # summary(Logistic_Model)$coefficients # S22: change the df per the summary stats above. Test_Dev = summary(Logistic_Model)$null.deviance - summary(Logistic_Model)$deviance 1-pchisq(q = Test_Dev, df = 10) ## [1] 0 #??? What does the 0 stands for here? significant? 7.3 c. (8 points) Conduct deviance tests with the drop1() function to assess the significance of each individual variable and report the results. Compare the p-values to those obtained from summary(). I conducted the deviance test using drop1 function. The pitch speed, plate_z, launch_angle, launch_speed and stand are significant at 95% confidence level. The p-value are different from the summary. Plate_z went from 0.145 to 0.001; Stand went 0.240 from to 0.01. All other variables have similar p values as before. I would trust the drop1 functions results as it doesnt rely on the unreliable walts test. # S30, S67: stepwise selection - backward. large predictors some will randomly be significant. but note, you are not eliminate the one with p at border? once your predictors number is small enough. it is ok? #??? Why no reaction for a long time? drop1(Logistic_Model,test=&quot;F&quot;) ## Single term deletions ## ## Model: ## home_run ~ pitch_speed + pitch_spin + pitch_horizontal_movement + ## pitch_vertical_movement + plate_x + plate_z + pitcher_hand + ## launch_angle + launch_speed + stand ## Df Deviance AIC F value Pr(&gt;F) ## &lt;none&gt; 6109.7 6131.7 ## pitch_speed 1 6147.7 6167.7 183.9705 &lt; 2.2e-16 *** ## pitch_spin 1 6110.0 6130.0 1.4772 0.224219 ## pitch_horizontal_movement 1 6110.0 6130.0 1.4068 0.235594 ## pitch_vertical_movement 1 6110.3 6130.3 2.7214 0.099019 . ## plate_x 1 6110.0 6130.0 1.3233 0.250003 ## plate_z 1 6111.8 6131.8 10.2774 0.001348 ** ## pitcher_hand 1 6110.1 6130.1 2.0781 0.149433 ## launch_angle 1 7716.6 7736.6 7781.3469 &lt; 2.2e-16 *** ## launch_speed 1 10133.9 10153.9 19486.6009 &lt; 2.2e-16 *** ## stand 1 6111.1 6131.1 6.6608 0.009861 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 7.4 d. (6 points) Fit a smaller model after removing all variables which are insignificant using  = 0.05. Compare this model to the larger model, are they significantly different? What are the implications of this with regard to model selection? Until the end of this question, use the smaller model for all analysis I compare the smaller model with the larger model, they are not significantly different per the deviance test. The result suggests that larger model doesnt add much explanation power. I should use the smaller model for parsimonious explanation. Logistic_Model_better &lt;- glm(formula = home_run ~ pitch_speed+ plate_z + launch_angle + launch_speed + stand, family = binomial, data = HomeRun) summary(Logistic_Model_better) ## ## Call: ## glm(formula = home_run ~ pitch_speed + plate_z + launch_angle + ## launch_speed + stand, family = binomial, data = HomeRun) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.7028 -0.1792 -0.0455 -0.0036 4.2511 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -32.484157 0.935687 -34.717 &lt;2e-16 *** ## pitch_speed -0.057392 0.006136 -9.354 &lt;2e-16 *** ## plate_z 0.096252 0.070659 1.362 0.173 ## launch_angle 0.098273 0.002941 33.420 &lt;2e-16 *** ## launch_speed 0.324831 0.007788 41.711 &lt;2e-16 *** ## standR -0.088306 0.069221 -1.276 0.202 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 10624.6 on 29595 degrees of freedom ## Residual deviance: 6111.3 on 29590 degrees of freedom ## AIC: 6123.3 ## ## Number of Fisher Scoring iterations: 9 #??? Is sthe plate_Z and standR just noise? random significant on the backward selection drop1 process? # Note the df difference 11 (baseline all variable) - 5 (better model) = 6 Test_Dev_better = summary(Logistic_Model_better)$deviance - summary(Logistic_Model)$deviance 1-pchisq(q = Test_Dev_better, df = 6) ## [1] 0.9502016 # The result shows insignificant difference between the two. Thus, better model is parsimony. 7.5 e. (7 points) How does the launch speed after the ball is hit affect the odds of HomeRun occurring? Provide a confidence interval for this value. The odds is 1.38. As the launch speed increase by 1, the chance of success will increase by 100% * (1.38-1) = 38% The confidence interval for the odds is between 1.36 to 1.40. # S57/ Coef_LogOdds &lt;- coefficients(Logistic_Model_better) Coef_LogOdds |&gt; exp() ## (Intercept) pitch_speed plate_z launch_angle launch_speed standR ## 7.803867e-15 9.442236e-01 1.101037e+00 1.103264e+00 1.383797e+00 9.154811e-01 #1.383797e+00 ??? interpret e+00 menas integer? 1.38 more success than failure to hit home rum #S17: When launch speed increases by 1, the odds of success increases by a factor of 1.38 pr 100% * (1.38-1) = 38% increase in chance/ odds??? of sucess. #What if I want the chance vs. odds??? # why the number different? 1.38/(1+1.38) = .57 # type of confidenec interval #??? What if I use directly the CI from the model? violate assumption in binomial family? # A lot of warning message: Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ??? #S44: #Confidence_Intervals &lt;- confint(Logistic_Model_better) #Confidence_Intervals Confidence_Intervals &lt;- confint(Logistic_Model_better) ## Waiting for profiling to be done... Confidence_Intervals |&gt; exp() ## 2.5 % 97.5 % ## (Intercept) 1.218007e-15 4.773109e-14 ## pitch_speed 9.329500e-01 9.556669e-01 ## plate_z 9.584535e-01 1.264379e+00 ## launch_angle 1.096995e+00 1.109715e+00 ## launch_speed 1.363138e+00 1.405399e+00 ## standR 7.995274e-01 1.048816e+00 7.6 f. (5 points) Using the deviance residuals, make a binned residual vs fitted probability plot and comment on the fit of the model. The model doesnt fit well. The residual gets bigger rather than smaller when the probability increases. Based on the theoretical distribution, as the probability increases, the variance should be smaller. Thus, there may be misspecification in the model. # bin plot vs. fitted plt # random subset is not required. library(tidyverse) plot_bin &lt;- function(Y, X, bins = 100, return.DF = FALSE){ Y_Name &lt;- deparse(substitute(Y)) X_Name &lt;- deparse(substitute(X)) Binned_Plot &lt;- data.frame(Plot_Y = Y, Plot_X = X) Binned_Plot$bin &lt;- cut(Binned_Plot$Plot_X,breaks = bins) |&gt; as.numeric() Binned_Plot_summary &lt;- Binned_Plot |&gt; group_by(bin) |&gt; summarise(Y_ave = mean(Plot_Y), X_ave = mean(Plot_X), Count = n()) |&gt; as.data.frame() plot(y = Binned_Plot_summary$Y_ave, x = Binned_Plot_summary$X_ave, ylab = Y_Name,xlab = X_Name) if(return.DF) return(Binned_Plot_summary) } #S32: vs. fitted probability plot rather than individual variable. ??? When to use the individual variable on? Logistic_better_Resids &lt;- residuals(Logistic_Model_better,type=&quot;response&quot;) Logistic_better_Predictions &lt;- predict(Logistic_Model_better,type = &quot;response&quot;) NumBins &lt;- 200 Binned_Data &lt;- plot_bin(Y = Logistic_better_Resids, X = Logistic_better_Predictions, bins = NumBins, return.DF = TRUE) abline(0,1,lty=2) # The model doesn&#39;t fit well??? why? #S28 ??? why deviance not the same. why no u shape. np vs. npq. #need more bins, the shape will follow theroy on S18 Chapter 2. 7.7 g. (4 points) Using a probability of 0.5 as the threshold for predicting an observation yielding a home run, create a table classifying the predictions against the observed values. Describe your findings. What is the misclassification rate? The model predicts well as the accuracy rate is high. The misclassification rate is 0.043 or 4%. However, the sensitivity and specificity are poor. # confusion matrix # S47: Predicted_Prob &lt;- predict(Logistic_Model_better, type=&quot;response&quot;) Predicted_Class &lt;- ifelse(Predicted_Prob &gt; 0.5,yes = &quot;home_run&quot;,no = &quot;no_home_run&quot;) Observed_Class &lt;- HomeRun$home_run table(Observed_Class,Predicted_Class) ## Predicted_Class ## Observed_Class home_run no_home_run ## 0 190 28113 ## 1 274 1019 # Misclassification rate: table_classify = table(Observed_Class,Predicted_Class) Accuracy_rate = (table_classify[2,1]+table_classify[1,2])/(table_classify[1,1]+table_classify[1,2]+table_classify[2,1]+table_classify[2,2]) 1-Accuracy_rate ## [1] 0.04085011 7.8 h. (6 points) Using probability thresholds from 0.005 to 0.995, obtain the sensitivities and specificity of the resulting predictions. Create an ROC plot and comment on the effectiveness of the models ability to correctly classify the response. As we vary the threshold to determine classifications, is inverse relationship between sensitivity and specificity strongly evident? The inverse relationship is strongly evident as the threshold increases. # Create ROC curve #S48: thresh &lt;- seq(0.005,0.995,0.001) Sensitivity &lt;- numeric(length(thresh)) Specificity &lt;- numeric(length(thresh)) for(j in seq(along=thresh)){ Predicted_Class &lt;- ifelse(Predicted_Prob &gt; thresh[j], yes = &quot;1_home_run&quot;,no = &quot;0_no_home_run&quot;) Conf_Matrix &lt;- table(Observed_Class,Predicted_Class) # Specificity[j] &lt;- Conf_Matrix[1,1]/(Conf_Matrix[1,1]+Conf_Matrix[1,2]) # Sensitivity[j] &lt;- Conf_Matrix[2,2]/(Conf_Matrix[2,1]+Conf_Matrix[2,2]) } plot(1-Specificity,Sensitivity,type=&quot;l&quot;,main = &quot;ROC curve&quot;, xlim = c(0,1),ylim = c(0,1)) abline(0,1,lty=2) 7.9 i. (5 points) Produce a plot of the sensitivity and specificity against the threshold. Is there a threshold for classification you would recommend that provides a good balance between the two? Make another confusion matrix using this cutoff, how does the result compare to the previous one? Consider the types of errors you observe. The optimal threshold is 0.05. The result is better than 0.5 as the cut-off: The accuracy is worse but sensitivity and specificity are higher or more discriminant. The accuracy rate is not a good indicator of model performance when the outcomes is rare such as home run. #! The result is better than 0.5 as the cut-off The accuracy is worse but senstivity and specificity is higher #S50 matplot(thresh,cbind(Sensitivity,Specificity),type=&quot;l&quot;, xlab=&quot;Threshold&quot;,ylab=&quot;Proportion&quot;,lty=1:2) # abline, v = cutoff. # Rewrie the answer # confusion matrix # when we lower the threshold, more on the home run category. if choose 0.5, for the rare. # S47: Predicted_Prob &lt;- predict(Logistic_Model_better, type=&quot;response&quot;) Predicted_Class &lt;- ifelse(Predicted_Prob &gt; 0.05,yes = &quot;home_run&quot;,no = &quot;no_home_run&quot;) Observed_Class &lt;- HomeRun$home_run table(Observed_Class,Predicted_Class) ## Predicted_Class ## Observed_Class home_run no_home_run ## 0 4222 24081 ## 1 1182 111 # Misclassification rate: table_classify = table(Observed_Class,Predicted_Class) Accuracy_rate = (table_classify[2,1]+table_classify[1,2])/(table_classify[1,1]+table_classify[1,2]+table_classify[2,1]+table_classify[2,2]) 1-Accuracy_rate ## [1] 0.1464049 7.10 j. (3 points) Consider a logistic model with only launch_angle and launch_speed being used to predict the probability of a home run. What is the AIC of this model? The AIC of the model is 6203. # Produce AIC Logistic_Model_reduced &lt;- glm(formula = home_run ~ launch_angle + launch_speed, family = binomial, data = HomeRun) summary(Logistic_Model_reduced) ## ## Call: ## glm(formula = home_run ~ launch_angle + launch_speed, family = binomial, ## data = HomeRun) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.7202 -0.1812 -0.0479 -0.0038 4.2110 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -36.664943 0.835606 -43.88 &lt;2e-16 *** ## launch_angle 0.097468 0.002849 34.22 &lt;2e-16 *** ## launch_speed 0.317479 0.007616 41.69 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 10624.6 on 29595 degrees of freedom ## Residual deviance: 6197.6 on 29593 degrees of freedom ## AIC: 6203.6 ## ## Number of Fisher Scoring iterations: 9 7.11 k. (11 points) Create a dummy variable which is 1 if launch_angle is between 20 and 40 degrees and use this variable in your model instead of the raw value for launch_angle. Then, complete the following: 1. Compare the AIC of this model to the model in part j, which model is better? 2. What does the coefficient of your dummy variable mean? Interpret the value. 3. Interpret the value of the intercept by converting to a probability. Does this result make sense? The model with dummys AIC is 4508 which is much smaller than the 6203. The dummy variable means when the launch_angle is between 20 and 40 degree the odds of success is higher than the launch_angle outside the window. Thus, there is a optimal point of the launch angle. The intercept is very small or 4.186275e-17 which means the chance of making a home run when all other variables are set to 0 is very small. 7.11.1 # compare with other temp, here is more likely. HomeRun$launch_angle_d &lt;- ifelse(HomeRun$launch_angle &gt;= 20 &amp; HomeRun$launch_angle &lt;= 40 ,1,0) # Produce AIC Logistic_Model_reduced_d &lt;- glm(formula = home_run ~ launch_angle_d + launch_speed, family = binomial, data = HomeRun) summary(Logistic_Model_reduced) ## ## Call: ## glm(formula = home_run ~ launch_angle + launch_speed, family = binomial, ## data = HomeRun) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.7202 -0.1812 -0.0479 -0.0038 4.2110 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -36.664943 0.835606 -43.88 &lt;2e-16 *** ## launch_angle 0.097468 0.002849 34.22 &lt;2e-16 *** ## launch_speed 0.317479 0.007616 41.69 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 10624.6 on 29595 degrees of freedom ## Residual deviance: 6197.6 on 29593 degrees of freedom ## AIC: 6203.6 ## ## Number of Fisher Scoring iterations: 9 summary(Logistic_Model_reduced_d) ## ## Call: ## glm(formula = home_run ~ launch_angle_d + launch_speed, family = binomial, ## data = HomeRun) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1862 -0.0780 -0.0148 -0.0025 3.9332 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -37.712135 0.991093 -38.05 &lt;2e-16 *** ## launch_angle_d 5.329550 0.156367 34.08 &lt;2e-16 *** ## launch_speed 0.313245 0.009108 34.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 10624.6 on 29595 degrees of freedom ## Residual deviance: 4502.2 on 29593 degrees of freedom ## AIC: 4508.2 ## ## Number of Fisher Scoring iterations: 9 Coef_LogOdds_d &lt;- coefficients(Logistic_Model_reduced_d) Coef_LogOdds_d |&gt; exp() ## (Intercept) launch_angle_d launch_speed ## 4.186275e-17 2.063451e+02 1.367857e+00 # convert intercept to probability odds/ (1+odds) (4.186275e-17)/(1-4.186275e-17) ## [1] 4.186275e-17 "],["question-2.html", "Chapter 8 Question 2 8.1 a. (5 points) Make a plot or set of tables showing the distribution of LSD use between genders and interpret. You can use code similar to that on slide 84 of the course notes. 8.2 b. (9 points) Fit a proportional odds model using LSD as the response variable and the other variables listed above as predictors. Using drop1(), test if the variables are significant or insignificant and describe 2 your results. 8.3 c. (6 points) Interpret the values of the intercepts j . 8.4 d. (10 points) Print the coefficient table and interpret the values of the significant personality characteristics. 8.5 e. (7250 only) (6 points) Explore interacting some of the categorical demographic variables with the personality measurements and report your findings, does the nature of any personality characteristics affect on LSD usage change according to your analysis?", " Chapter 8 Question 2 2) Ordinal Regression The DrugConsumption.csv file on Canvas contains records for 1458 respondents. For each respondent 12 attributes are known: Personality measurements which include NEO-FFI-R (neuroticism, extraversion, openness to experience, agreeableness, and conscientiousness), BIS-11 (impulsivity), and ImpSS (sensation seeking), level of education, age, gender and country of residence. In addition, participants were questioned concerning their use of 18 legal and illegal drugs, of which only LSD is included for our study. The variables are as follows:  LSD: Target variable, ordinal with levels CL0-CL5, which correspond to Never Used, Used over a Decade Ago, Used in Last Decade, Used in Last Year, Used in Last Month, Used in Last Week.  Age: Categorical age  Gender: Gender  Education: Categorical Educational level  Country: UK or USA  Nscore: NEO-FFI-R, a psychological measure of Neuroticism  Escore: NEO-FFI-R Extraversion.  Oscore: NEO-FFI-R Openness to experience  Ascore: NEO-FFI-R Agreeableness  Cscore: NEO-FFI-R Conscientiousness  Impulsive: Impulsiveness score measured by BIS-11  SS: Impulsive Sensation Seeking measured by ImpSS You may read the data as follows: Drugs &lt;- read.csv(/DrugConsumption.csv, stringsAsFactors = TRUE) Drugs$LSD &lt;- ordered(Drugs$LSD,levels = c(paste0(CL,0:5))) Drugs &lt;- read.csv(&quot;E:/Cloud/OneDrive - University of Missouri/Mizzou_PhD/Class plan/Applied Stats Model II/HW1/DrugConsumption.csv&quot;, stringsAsFactors = TRUE) #install.packages(&quot;tidyverse&quot;) library(tidyverse) # for mutate function #??? the following code gave me error. What is it? #Drugs$LSD &lt;- ordered(Drugs$LSD,levels = c(paste0(&quot;CL&quot;,0:5))) 8.1 a. (5 points) Make a plot or set of tables showing the distribution of LSD use between genders and interpret. You can use code similar to that on slide 84 of the course notes. I re-coded the categories into descriptive names Used over a Decade Ago, Used in Last Decade, Used in Last Year, Used in Last Month, Used in Last Week. Based on the plot, male are more likely to use drugs than female. Many female never used any drugs. Drugs$LSD_o &lt;- NA Drugs$LSD_o[Drugs$LSD %in%c(&quot;CL0&quot;)] &lt;- &quot;Never Used&quot; Drugs$LSD_o[Drugs$LSD %in%c(&quot;CL1&quot;)] &lt;- &quot;Used over a Decade Ago&quot; Drugs$LSD_o[Drugs$LSD %in%c(&quot;CL2&quot;)] &lt;- &quot;Used in Last Decade&quot; Drugs$LSD_o[Drugs$LSD %in%c(&quot;CL3&quot;)] &lt;- &quot;Used in Last Year&quot; Drugs$LSD_o[Drugs$LSD %in%c(&quot;CL4&quot;)] &lt;- &quot;Used in Last Month&quot; Drugs$LSD_o[Drugs$LSD %in%c(&quot;CL5&quot;)] &lt;- &quot;Used in Last Week&quot; table(Drugs$LSD_o) ## ## Never Used Used in Last Decade Used in Last Month ## 866 134 62 ## Used in Last Week Used in Last Year Used over a Decade Ago ## 40 146 210 Plot_DF &lt;- Drugs |&gt; group_by(Gender, LSD_o) |&gt; summarise(count=n()) |&gt; group_by(Gender) |&gt; mutate(etotal=sum(count), proportion=count/etotal) ## `summarise()` has grouped output by &#39;Gender&#39;. You can override using the ## `.groups` argument. # mutate(Age_Grp=cut_number(age,4)) |&gt; not used since the gender is not continous. # Plot_DF -&gt; only a dataframe Gender_Plot &lt;- ggplot(Plot_DF, aes(x=Gender, y=proportion, group=LSD_o, linetype=LSD_o))+ geom_line() Gender_Plot 8.2 b. (9 points) Fit a proportional odds model using LSD as the response variable and the other variables listed above as predictors. Using drop1(), test if the variables are significant or insignificant and describe 2 your results. The significant variables are age 55+, GenderMale, CountryUSA, Oscore and Cscore. The drop1 function shows gender, country, Oscore and Cscore are significant but not the age 55+. Strangely the drop1 function doesnt factorize the categorical variables such as age and education. Overall, the two results are quite consistent with each other. library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select Drugs$LSD_o &lt;- ordered(Drugs$LSD_o, levels=c(&quot;Never Used&quot;, &quot;Used over a Decade Ago&quot;, &quot;Used in Last Decade&quot;, &quot;Used in Last Year&quot;, &quot;Used in Last Month&quot;, &quot;Used in Last Week&quot;)) PropOdds_Model &lt;- MASS::polr(LSD_o ~ Age + Gender + Education + Country + Nscore + Escore + Oscore + Ascore + Cscore, Drugs) summary(PropOdds_Model) ## ## Re-fitting to get Hessian ## Call: ## MASS::polr(formula = LSD_o ~ Age + Gender + Education + Country + ## Nscore + Escore + Oscore + Ascore + Cscore, data = Drugs) ## ## Coefficients: ## Value Std. Error t value ## Age25-34 -0.097001 0.16037 -0.60486 ## Age35-44 -0.006247 0.17504 -0.03569 ## Age45-54 -0.218738 0.18630 -1.17412 ## Age55+ -0.671385 0.26083 -2.57408 ## GenderMale 0.773020 0.12094 6.39159 ## EducationHighSchool -0.105167 0.29551 -0.35588 ## EducationHSDropout 0.124446 0.22553 0.55180 ## EducationPostGraduate -0.294833 0.17637 -1.67163 ## EducationProfessionalCert 0.011469 0.19528 0.05873 ## EducationSomeCollege 0.078480 0.16218 0.48392 ## CountryUSA 1.708138 0.13773 12.40252 ## Nscore -0.110185 0.06866 -1.60469 ## Escore 0.014487 0.06745 0.21478 ## Oscore 0.597784 0.06685 8.94237 ## Ascore -0.018913 0.05870 -0.32221 ## Cscore -0.178554 0.06696 -2.66662 ## ## Intercepts: ## Value Std. Error t value ## Never Used|Used over a Decade Ago 1.2133 0.1750 6.9337 ## Used over a Decade Ago|Used in Last Decade 2.1296 0.1832 11.6275 ## Used in Last Decade|Used in Last Year 2.9048 0.1887 15.3969 ## Used in Last Year|Used in Last Month 4.1597 0.2054 20.2562 ## Used in Last Month|Used in Last Week 5.2439 0.2415 21.7105 ## ## Residual Deviance: 3143.917 ## AIC: 3185.917 #problen with converge if I include the same variable LSD. need to remove it. drop1(PropOdds_Model,test=&quot;Chisq&quot;) ## Single term deletions ## ## Model: ## LSD_o ~ Age + Gender + Education + Country + Nscore + Escore + ## Oscore + Ascore + Cscore ## Df AIC LRT Pr(&gt;Chi) ## &lt;none&gt; 3185.9 ## Age 4 3186.2 8.270 0.082171 . ## Gender 1 3225.3 41.410 1.234e-10 *** ## Education 5 3181.0 5.122 0.401142 ## Country 1 3343.6 159.732 &lt; 2.2e-16 *** ## Nscore 1 3186.5 2.573 0.108671 ## Escore 1 3184.0 0.046 0.829943 ## Oscore 1 3266.8 82.918 &lt; 2.2e-16 *** ## Ascore 1 3184.0 0.104 0.747364 ## Cscore 1 3191.1 7.183 0.007359 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # drop1(PropOdds_Model) # why is the age not broken down as factors? # Drop1 will drop all in age variable. 8.3 c. (6 points) Interpret the values of the intercepts j . The intercepts are listed as below: Never Used|Used over a Decade Ago 3.364569 . It means cumulative odds of being never used is 3.36 when all other variables are set to 0. Used over a Decade Ago|Used in Last Decade 8.411502 It means cumulative odds of being Used in last decade is 8.411502 when all other variables are set to 0. Used in Last Decade|Used in Last Year 18.26159 It means cumulative odds of being Used in last year is 18.2615 when all other variables are set to 0. Used in Last Year|Used in Last Month 64.0523 It means cumulative odds of being Used in last month is 64.0523 when all other variables are set to 0. Used in Last Month|Used in Last Week 189.4074 It means cumulative odds of being Used in last week is 189 when all other variables are set to 0. PropOdds_Model_sum &lt;- summary(PropOdds_Model) ## ## Re-fitting to get Hessian #PropOdds_Model_sum$coefficients exp(1.2133) ## [1] 3.364569 exp(2.1296) ## [1] 8.411502 exp(2.9048) ## [1] 18.26159 exp(4.1597) ## [1] 64.0523 exp(5.2439) ## [1] 189.4074 #!not sure how to explain it. S97. Does it also needs a exp()? -&gt; how to extract it automatically? does it mean % of ppl??? 8.4 d. (10 points) Print the coefficient table and interpret the values of the significant personality characteristics. The open to experience increase the odds of using the drugs by 1.81. Conscientiousness decreases the odds of using the drugs by 0.8. coefficients(PropOdds_Model) ## Age25-34 Age35-44 Age45-54 ## -0.097000990 -0.006246867 -0.218738114 ## Age55+ GenderMale EducationHighSchool ## -0.671384707 0.773019582 -0.105166700 ## EducationHSDropout EducationPostGraduate EducationProfessionalCert ## 0.124446396 -0.294833364 0.011469188 ## EducationSomeCollege CountryUSA Nscore ## 0.078480163 1.708138461 -0.110184787 ## Escore Oscore Ascore ## 0.014486532 0.597783608 -0.018912601 ## Cscore ## -0.178553588 #??? are the coefficient intercept or beta? #Both personality significant. The higher the less likely to move ?!!! #Oscore exp(0.597784) ## [1] 1.818085 #Cscore exp(-0.178554) ## [1] 0.8364789 8.5 e. (7250 only) (6 points) Explore interacting some of the categorical demographic variables with the personality measurements and report your findings, does the nature of any personality characteristics affect on LSD usage change according to your analysis? I tested the interaction between demographic variables (age, gender and education) and personality measurements (Oscore, Cscore). I found the following significant interactions: Age35-44:Oscore -0.396513 Age25-34:Cscore -0.368074 Age35-44:Cscore -0.590439 Age45-54:Cscore -0.587787 Age55+:Cscore -0.614652 The results have a consistent theme that more mature people (age above 18-24) and higher on open to experience or conscientiousness scores are less likely to use drugs. PropOdds_Model_int &lt;- MASS::polr(LSD_o ~ Age + Gender + Education + Country + Nscore + Escore + Oscore + Ascore + Cscore + Age*Oscore + Age*Cscore + Gender*Oscore + Gender*Cscore + Education*Oscore + Education *Cscore, Drugs) summary(PropOdds_Model_int) ## ## Re-fitting to get Hessian ## Call: ## MASS::polr(formula = LSD_o ~ Age + Gender + Education + Country + ## Nscore + Escore + Oscore + Ascore + Cscore + Age * Oscore + ## Age * Cscore + Gender * Oscore + Gender * Cscore + Education * ## Oscore + Education * Cscore, data = Drugs) ## ## Coefficients: ## Value Std. Error t value ## Age25-34 -0.172081 0.17273 -0.99623 ## Age35-44 -0.104423 0.18315 -0.57016 ## Age45-54 -0.280105 0.19515 -1.43533 ## Age55+ -0.654289 0.26780 -2.44318 ## GenderMale 0.774285 0.12648 6.12198 ## EducationHighSchool 0.019760 0.32261 0.06125 ## EducationHSDropout -0.102915 0.24983 -0.41194 ## EducationPostGraduate -0.306062 0.19143 -1.59878 ## EducationProfessionalCert 0.046031 0.20393 0.22572 ## EducationSomeCollege 0.070706 0.17446 0.40528 ## CountryUSA 1.744167 0.14162 12.31579 ## Nscore -0.118884 0.06971 -1.70541 ## Escore 0.038420 0.06843 0.56143 ## Oscore 0.685866 0.17126 4.00483 ## Ascore -0.018826 0.06047 -0.31131 ## Cscore 0.116770 0.16389 0.71247 ## Age25-34:Oscore 0.004876 0.17067 0.02857 ## Age35-44:Oscore -0.396513 0.17969 -2.20666 ## Age45-54:Oscore -0.143649 0.21006 -0.68386 ## Age55+:Oscore -0.227344 0.27432 -0.82876 ## Age25-34:Cscore -0.368074 0.16557 -2.22312 ## Age35-44:Cscore -0.590439 0.17650 -3.34527 ## Age45-54:Cscore -0.587787 0.19676 -2.98727 ## Age55+:Cscore -0.614652 0.29484 -2.08467 ## GenderMale:Oscore 0.009991 0.12113 0.08248 ## GenderMale:Cscore -0.052592 0.12510 -0.42040 ## EducationHighSchool:Oscore -0.293782 0.27753 -1.05855 ## EducationHSDropout:Oscore -0.255640 0.25420 -1.00565 ## EducationPostGraduate:Oscore 0.058642 0.19749 0.29693 ## EducationProfessionalCert:Oscore 0.303077 0.21251 1.42618 ## EducationSomeCollege:Oscore 0.012843 0.16599 0.07737 ## EducationHighSchool:Cscore 0.056019 0.32688 0.17138 ## EducationHSDropout:Cscore -0.226221 0.24937 -0.90717 ## EducationPostGraduate:Cscore 0.254668 0.19192 1.32694 ## EducationProfessionalCert:Cscore 0.101606 0.21581 0.47082 ## EducationSomeCollege:Cscore 0.026077 0.16748 0.15571 ## ## Intercepts: ## Value Std. Error t value ## Never Used|Used over a Decade Ago 1.1821 0.1815 6.5142 ## Used over a Decade Ago|Used in Last Decade 2.1166 0.1893 11.1805 ## Used in Last Decade|Used in Last Year 2.9087 0.1952 14.9009 ## Used in Last Year|Used in Last Month 4.1936 0.2136 19.6321 ## Used in Last Month|Used in Last Week 5.2923 0.2499 21.1740 ## ## Residual Deviance: 3108.449 ## AIC: 3190.449 STAT 4520/7520 - Homework 2 Spring 2022 Due: March 6, 2022 The French Motor Third-Party Liability Claims dataset is located in Insurance.csv and contains records on insurance policies. A claim is the request made by a policyholder to the insurer to compensate for a loss covered by the insurance. Insurance companies are interested in modeling how much they are expected to payout per year. In other words the mean of the total claim amount per year also referred to as the pure premium. The varibles in the dataset are as follows: Feature variables:  Area: The area code.  VehPower: The power of the car (ordered categorical but treat as numeric).  VehAge: The vehicle age, in years.  DrivAge: The driver age, in years.  BonusMalus: Bonus/malus, between 50 and 350 and relating to the carbon emissions of the vehicle: &lt; 100 means bonus(tax credit), &gt; 100 means malus(tax penalty).  VehBrand: The car brand (unknown categories).  VehGas: The car gas, Diesel or regular.  Density: The (log of) density of inhabitants (number of inhabitants per km2) in the city the driver of the car lives in.  Region: The policy regions in France (based on a standard French classification). Target variables:  ClaimNb: Total number of claims on the policy.  Exposure: Number of years the policy has been held.  Frequency: Average number of claims per year on the policy.  AvgClaimAmount: Average amount paid out per claim in the policy.  PurePremium: Expected total claim amount per year for each policy. There are several ways to create these predictions, we could: 1. Model the number of claims with a count model, and separately the average claim amount per claim and multiply the predictions of both in order to get the total claim amount. 2. Model the total claim amount per exposure directly, typically with a Tweedie distribution. We will explore both of these options here. The dataset can be loaded for properly formatted with the commands: Insurance &lt;- read.csv(&quot;E:/Cloud/OneDrive - University of Missouri/Mizzou_PhD/Class plan/Applied Stats Model II/HW2/Insurance.csv&quot;, stringsAsFactors = TRUE) Insurance$VehBrand &lt;- as.factor(Insurance$VehBrand) Insurance$VehPower &lt;- as.factor(Insurance$VehPower) Insurance$VehGas &lt;- as.factor(Insurance$VehGas) Insurance$Region &lt;- as.factor(Insurance$Region) Insurance$Area &lt;- as.factor(Insurance$Area) "],["modeling-the-number-of-claims.html", "Chapter 9 1 1) Modeling the Number of Claims 9.1 a. (5 points) Create a table to present the distribution of the number of claims by computing the proportion of each possibility. What percent of the policies have 0 claims? 9.2 b. (6 points) Create a Poisson regression to predict the number of claims in the year using only the variables for vehicle age, driver age, bonusMalus, and density. What is the deviance? Does is differ significantly from the null deviance? 9.3 c. (4 points) Generate predictions on the response scale µ and round them to the nearest count. Create a table as in part a and comment on how similar or dissimilar this result is. 9.4 d. (3 points) Fit a negative binomial model to the data and generate predictions as in part c. Did this solve the problem? 9.5 e. (5 points) Now fit a zero inflation Poisson model (ZIP) and compute the fitted proportion of zero counts as on slide 38 of the Poisson Notes. Does this help? 9.6 f. (8 points) Interpret the signs of the coefficient estimates out of the ZIP model, both the count portion as well as the zero inflated portion!", " Chapter 9 1 1) Modeling the Number of Claims 9.1 a. (5 points) Create a table to present the distribution of the number of claims by computing the proportion of each possibility. What percent of the policies have 0 claims? Plot_DF ## # A tibble: 4 x 4 ## ClaimNb count etotal proportion ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 0 42685 44999 0.949 ## 2 1 2224 44999 0.0494 ## 3 2 84 44999 0.00187 ## 4 3 6 44999 0.000133 The result shows that about 95% of the policies have 0 claims. 9.2 b. (6 points) Create a Poisson regression to predict the number of claims in the year using only the variables for vehicle age, driver age, bonusMalus, and density. What is the deviance? Does is differ significantly from the null deviance? Poisson_Mod &lt;- glm(ClaimNb ~ VehAge + DrivAge + BonusMalus + Density, family=poisson, Insurance) summary(Poisson_Mod) ## ## Call: ## glm(formula = ClaimNb ~ VehAge + DrivAge + BonusMalus + Density, ## family = poisson, data = Insurance) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.1306 -0.3375 -0.3039 -0.2816 4.3804 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.9772852 0.1330830 -37.400 &lt; 2e-16 *** ## VehAge 0.0005833 0.0035970 0.162 0.871 ## DrivAge 0.0110918 0.0014819 7.485 7.17e-14 *** ## BonusMalus 0.0196450 0.0010601 18.532 &lt; 2e-16 *** ## Density 0.0529343 0.0111655 4.741 2.13e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 14381 on 44998 degrees of freedom ## Residual deviance: 14049 on 44994 degrees of freedom ## AIC: 18744 ## ## Number of Fisher Scoring iterations: 6 # S14 pchisq(summary(Poisson_Mod)$deviance,Poisson_Mod$df.residual, lower.tail = FALSE) ## [1] 1 The deviance is 14049. It is not statistically different from the null model. 9.3 c. (4 points) Generate predictions on the response scale µ and round them to the nearest count. Create a table as in part a and comment on how similar or dissimilar this result is. Predicted_Means &lt;- predict(Poisson_Mod,type = &quot;response&quot;) #X2 &lt;- sum((Insurance$ClaimNb - Predicted_Means)2/Predicted_Means) Insurance$Predicted_Class &lt;- round(Predicted_Means,0) #count(Insurance, Predicted_Class) The result is not very similar. There are way more 0 then the actual data. 9.4 d. (3 points) Fit a negative binomial model to the data and generate predictions as in part c. Did this solve the problem? #S26 NegBinom_Mod &lt;- MASS::glm.nb(ClaimNb ~ VehAge + DrivAge + BonusMalus + Density,Insurance) # Convert back to glm() style object NegBinom_Mod_glm &lt;- MASS::glm.convert(NegBinom_Mod) summary(NegBinom_Mod_glm, dispersion=1) ## ## Call: ## stats::glm(formula = ClaimNb ~ VehAge + DrivAge + BonusMalus + ## Density, data = Insurance, family = negative.binomial(theta = 2.89746020004541, ## link = log)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.0799 -0.3360 -0.3026 -0.2805 4.1113 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.9864644 0.1358338 -36.710 &lt; 2e-16 *** ## VehAge 0.0005578 0.0036355 0.153 0.878 ## DrivAge 0.0111508 0.0015060 7.404 1.32e-13 *** ## BonusMalus 0.0197329 0.0010930 18.053 &lt; 2e-16 *** ## Density 0.0531295 0.0112863 4.707 2.51e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(2.8975) family taken to be 1) ## ## Null deviance: 13623 on 44998 degrees of freedom ## Residual deviance: 13298 on 44994 degrees of freedom ## AIC: 18738 ## ## Number of Fisher Scoring iterations: 1 Predicted_Means_NB &lt;- predict(NegBinom_Mod,type = &quot;response&quot;) Insurance$Predicted_Class_NB &lt;- round(Predicted_Means_NB,0) #count(Insurance, Predicted_Class_NB) The negative Binomial result is quite similar to the Possion. Thus, it still has the issue of over-predicting 0s. 9.5 e. (5 points) Now fit a zero inflation Poisson model (ZIP) and compute the fitted proportion of zero counts as on slide 38 of the Poisson Notes. Does this help? #S34 #install.packages(pscl) library(pscl) ## Classes and Methods for R developed in the ## Political Science Computational Laboratory ## Department of Political Science ## Stanford University ## Simon Jackman ## hurdle and zeroinfl functions by Achim Zeileis ZIP_Mod &lt;- zeroinfl(ClaimNb ~ VehAge + DrivAge + BonusMalus + Density,Insurance) summary(ZIP_Mod) ## ## Call: ## zeroinfl(formula = ClaimNb ~ VehAge + DrivAge + BonusMalus + Density, ## data = Insurance) ## ## Pearson residuals: ## Min 1Q Median 3Q Max ## -0.7772 -0.2379 -0.2141 -0.1965 14.0577 ## ## Count model coefficients (poisson with log link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.350206 0.244515 -17.791 &lt; 2e-16 *** ## VehAge 0.055968 0.009113 6.141 8.17e-10 *** ## DrivAge 0.006835 0.002815 2.428 0.0152 * ## BonusMalus 0.015519 0.001764 8.796 &lt; 2e-16 *** ## Density 0.011674 0.019009 0.614 0.5391 ## ## Zero-inflation model coefficients (binomial with logit link): ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.24320 0.90679 0.268 0.78855 ## VehAge 0.20267 0.02203 9.198 &lt; 2e-16 *** ## DrivAge -0.02088 0.01168 -1.789 0.07368 . ## BonusMalus -0.01744 0.00691 -2.524 0.01160 * ## Density -0.17969 0.06359 -2.826 0.00471 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Number of iterations in BFGS optimization: 33 ## Log-likelihood: -9333 on 10 Df #S38 Pred_zero &lt;- predict(ZIP_Mod,type=&quot;zero&quot;) Pred_Count &lt;- predict(ZIP_Mod,type=&quot;count&quot;) Total_Prob0 &lt;- Pred_zero + (1-Pred_zero)*exp(-Pred_Count) mean(Total_Prob0) ## [1] 0.9487571 #count(Pred_zero) Now it helps. The proportion of 0 is close to the actual data. Moreover, there are more other count (i.e., &gt;0) as well. 9.6 f. (8 points) Interpret the signs of the coefficient estimates out of the ZIP model, both the count portion as well as the zero inflated portion! data.frame(exp.coef = exp(coef(ZIP_Mod))) ## exp.coef ## count_(Intercept) 0.01290416 ## count_VehAge 1.05756403 ## count_DrivAge 1.00685853 ## count_BonusMalus 1.01563970 ## count_Density 1.01174223 ## zero_(Intercept) 1.27532174 ## zero_VehAge 1.22467152 ## zero_DrivAge 0.97933313 ## zero_BonusMalus 0.98271136 ## zero_Density 0.83552783 #??? why it cannot run as S36 The higher the driver age and bonus miles and density reduce the likelihood of fire a claim. The increase in vehicle age increases the likelihood to fire a claim. The higher the driver age and bonus miles and density increases the number of claims being fired once they fire for a claim. The vehicle age also increases the number of claim to be fired. "],["modeling-the-average-claim-payout.html", "Chapter 10 2) Modeling the Average Claim Payout 10.1 a. (3 points) In this problem we will focus on modeling the positive continuous variable AvgClaimAmount. Attempt to fit a Gamma regression with a log link, predicting AvgClaimAmount as a function of all feature variables in the dataset. What happens when you attempt to fit this model? 10.2 b. (5 points) Create a new dataset consisting of the rows that correspond to strictly positive realizations of AvgClaimAmount. Make a histogram of this variable on standard and log scale and describe your findings. 10.3 c. (4 points) Fit the Gamma regression proposed in part a to the filtered dataset. Do you notice anything strange? How many iterations did it take glm() to find this result? 10.4 d. (3 points) To the glm() function, add the argument control = list(maxit = 500). What do you think this will do? Fit the model and examine the summary output to check the number of iterations needed for convergence. 10.5 e.(3 points) From the model in e. Interpret the parameter estimate for a vehicles age", " Chapter 10 2) Modeling the Average Claim Payout 10.1 a. (3 points) In this problem we will focus on modeling the positive continuous variable AvgClaimAmount. Attempt to fit a Gamma regression with a log link, predicting AvgClaimAmount as a function of all feature variables in the dataset. What happens when you attempt to fit this model? The model cannot run since 0 is not positive value. 10.2 b. (5 points) Create a new dataset consisting of the rows that correspond to strictly positive realizations of AvgClaimAmount. Make a histogram of this variable on standard and log scale and describe your findings. #??? do I need to bin it? #42: hist(Insurance$AvgClaimAmount[Insurance$AvgClaimAmount&gt;0]) hist(log(Insurance$AvgClaimAmount[Insurance$AvgClaimAmount&gt;0])) The distribution is highly skewed to the right. Once I use the log scale, it is more normally distributed. 10.3 c. (4 points) Fit the Gamma regression proposed in part a to the filtered dataset. Do you notice anything strange? How many iterations did it take glm() to find this result? # how to stop the code from running when it is stuck -&gt; Use the source code mode not the visual # run the gamma Insurance_GammaGLM &lt;- glm(AvgClaimAmount ~ ., family = Gamma(link = log),data = Insurance_filter) #summary(Insurance_GammaGLM)$dispersion It only shows not converged, so I am not sure how many iterations. 10.4 d. (3 points) To the glm() function, add the argument control = list(maxit = 500). What do you think this will do? Fit the model and examine the summary output to check the number of iterations needed for convergence. # max iteration so that more chance to converge # run the gamma Insurance_GammaGLM &lt;- glm(AvgClaimAmount ~ ., family = Gamma(link = log), control = list(maxit = 900),data = Insurance_filter) #summary(Insurance_GammaGLM)$dispersion summary(Insurance_GammaGLM) ## ## Call: ## glm(formula = AvgClaimAmount ~ ., family = Gamma(link = log), ## data = Insurance_filter, control = list(maxit = 900)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -16.2061 -0.7292 -0.2798 0.0261 6.3709 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.281e+00 3.920e-01 16.024 &lt; 2e-16 *** ## ClaimNb 1.477e-01 1.419e-01 1.041 0.298012 ## Exposure -4.400e-01 1.095e-01 -4.020 6.01e-05 *** ## AreaB -3.133e-01 1.453e-01 -2.155 0.031237 * ## AreaC -2.948e-01 1.932e-01 -1.526 0.127122 ## AreaD -6.100e-01 2.942e-01 -2.074 0.038213 * ## AreaE -8.155e-01 3.902e-01 -2.090 0.036733 * ## AreaF -1.277e+00 5.543e-01 -2.304 0.021300 * ## VehPower5 4.092e-02 1.078e-01 0.380 0.704202 ## VehPower6 1.642e-01 1.031e-01 1.593 0.111316 ## VehPower7 -3.993e-02 1.024e-01 -0.390 0.696563 ## VehPower8 -4.833e-02 1.577e-01 -0.306 0.759321 ## VehPower9 -7.725e-03 1.616e-01 -0.048 0.961883 ## VehPower10 1.492e-01 1.759e-01 0.848 0.396438 ## VehPower11 3.790e-01 2.410e-01 1.573 0.115900 ## VehPower12 3.186e-01 3.310e-01 0.963 0.335823 ## VehPower13 1.404e-01 4.039e-01 0.348 0.728199 ## VehPower14 -1.058e+00 8.402e-01 -1.259 0.208276 ## VehPower15 4.558e-01 5.979e-01 0.762 0.445974 ## VehAge -2.100e-02 6.551e-03 -3.206 0.001363 ** ## DrivAge 5.308e-03 2.390e-03 2.221 0.026466 * ## BonusMalus 6.340e-03 1.742e-03 3.640 0.000279 *** ## VehBrandB10 2.010e-02 1.998e-01 0.101 0.919877 ## VehBrandB11 2.283e-01 2.390e-01 0.955 0.339649 ## VehBrandB12 -1.283e-01 1.433e-01 -0.895 0.370723 ## VehBrandB13 3.616e-01 2.233e-01 1.619 0.105495 ## VehBrandB14 2.590e-01 3.274e-01 0.791 0.429056 ## VehBrandB2 1.433e-02 7.957e-02 0.180 0.857078 ## VehBrandB3 -1.154e-01 1.088e-01 -1.061 0.288934 ## VehBrandB4 -1.700e-01 1.473e-01 -1.154 0.248656 ## VehBrandB5 1.191e-01 1.303e-01 0.914 0.360851 ## VehBrandB6 -3.775e-02 1.352e-01 -0.279 0.780187 ## VehGasRegular -3.469e-03 6.661e-02 -0.052 0.958473 ## Density 1.588e-01 7.322e-02 2.169 0.030185 * ## RegionR21 3.089e-02 6.579e-01 0.047 0.962553 ## RegionR22 1.837e-02 2.976e-01 0.062 0.950778 ## RegionR23 7.812e-02 5.007e-01 0.156 0.876032 ## RegionR24 1.612e-01 1.583e-01 1.018 0.308649 ## RegionR25 -2.428e-01 2.707e-01 -0.897 0.369889 ## RegionR26 -7.689e-02 3.279e-01 -0.234 0.814641 ## RegionR31 8.568e-02 2.292e-01 0.374 0.708621 ## RegionR41 2.435e-01 2.294e-01 1.062 0.288535 ## RegionR42 1.646e-01 3.873e-01 0.425 0.670974 ## RegionR43 2.754e-01 1.441e+00 0.191 0.848487 ## RegionR52 2.032e-02 1.826e-01 0.111 0.911398 ## RegionR53 1.453e-01 1.792e-01 0.811 0.417344 ## RegionR54 1.153e-01 2.069e-01 0.557 0.577427 ## RegionR72 1.186e-01 2.011e-01 0.590 0.555270 ## RegionR73 4.518e-01 3.894e-01 1.160 0.246055 ## RegionR74 -2.587e-02 2.827e-01 -0.091 0.927106 ## RegionR82 1.446e-01 1.579e-01 0.916 0.359916 ## RegionR83 -3.973e-02 6.592e-01 -0.060 0.951941 ## RegionR91 3.616e-02 2.471e-01 0.146 0.883662 ## RegionR93 4.024e-01 1.743e-01 2.308 0.021073 * ## RegionR94 -3.099e-01 8.420e-01 -0.368 0.712912 ## PurePremium 7.572e-06 8.062e-08 93.925 &lt; 2e-16 *** ## Frequency -1.828e-02 3.574e-03 -5.113 3.43e-07 *** ## Predicted_Class -7.639e-01 5.701e-01 -1.340 0.180390 ## Predicted_Class_NB NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 2.03893) ## ## Null deviance: 3615.1 on 2313 degrees of freedom ## Residual deviance: 2588.4 on 2256 degrees of freedom ## AIC: 38837 ## ## Number of Fisher Scoring iterations: 776 I set the iterations maximum to be 900 since it used 777 iterations to get a converge. 10.5 e.(3 points) From the model in e. Interpret the parameter estimate for a vehicles age data.frame(exp.coef = exp(coef(Insurance_GammaGLM))) ## exp.coef ## (Intercept) 534.4196081 ## ClaimNb 1.1592162 ## Exposure 0.6440270 ## AreaB 0.7310581 ## AreaC 0.7446964 ## AreaD 0.5433356 ## AreaE 0.4424387 ## AreaF 0.2788054 ## VehPower5 1.0417724 ## VehPower6 1.1785087 ## VehPower7 0.9608558 ## VehPower8 0.9528167 ## VehPower9 0.9923051 ## VehPower10 1.1609080 ## VehPower11 1.4608336 ## VehPower12 1.3752012 ## VehPower13 1.1507308 ## VehPower14 0.3472931 ## VehPower15 1.5773632 ## VehAge 0.9792145 ## DrivAge 1.0053219 ## BonusMalus 1.0063602 ## VehBrandB10 1.0203055 ## VehBrandB11 1.2564385 ## VehBrandB12 0.8795597 ## VehBrandB13 1.4355945 ## VehBrandB14 1.2955724 ## VehBrandB2 1.0144353 ## VehBrandB3 0.8909914 ## VehBrandB4 0.8436909 ## VehBrandB5 1.1264397 ## VehBrandB6 0.9629577 ## VehGasRegular 0.9965371 ## Density 1.1721185 ## RegionR21 1.0313717 ## RegionR22 1.0185448 ## RegionR23 1.0812541 ## RegionR24 1.1748965 ## RegionR25 0.7844376 ## RegionR26 0.9259918 ## RegionR31 1.0894601 ## RegionR41 1.2757077 ## RegionR42 1.1788812 ## RegionR43 1.3169937 ## RegionR52 1.0205296 ## RegionR53 1.1564304 ## RegionR54 1.1222285 ## RegionR72 1.1259434 ## RegionR73 1.5711734 ## RegionR74 0.9744611 ## RegionR82 1.1555542 ## RegionR83 0.9610479 ## RegionR91 1.0368235 ## RegionR93 1.4954198 ## RegionR94 0.7335559 ## PurePremium 1.0000076 ## Frequency 0.9818884 ## Predicted_Class 0.4658244 ## Predicted_Class_NB NA As the vehicles age increases, the claim amount decreases by 3%. #??? Not make sense? how could the older car requires less claim? new car costs more to repair? "],["tweedie.html", "Chapter 11 3. Tweedie 11.1 a. (8 points) Using the entire dataset, create a profile likelihood of the index values p for use in a glm utilizing the Tweedie distribution to predict PurePremium directly. Make a plot of the profile likelihood and select the best value. (7520 - 3 points): For graduate students, I expect some exploration to find the best value as these data are quite poorly behaved. Look at the Value section of ?tweedie.profile for some hints, and I would suggest narrowing the search space for p and evaluating it on a somewhat fine grid. 11.2 b. Fit the Tweedie GLM model using the optimal power value you computed from the previous question. The file Insurance_test.csv contains a few additional observations which were not a portion of the data used to fit the previous models. We can read it in and format it using syntax similar to when we started. NewDataPoints &lt;- read.csv(./Insurance_test.csv) NewDataPoints$VehBrand &lt;- as.factor(NewDataPoints$VehBrand) NewDataPoints$VehPower &lt;- as.factor(NewDataPoints$VehPower) NewDataPoints$VehGas &lt;- as.factor(NewDataPoints$VehGas) NewDataPoints$Region &lt;- as.factor(NewDataPoints$Region) NewDataPoints$Area &lt;- as.factor(NewDataPoints$Area) 11.3 c.(7 points) Use your Poisson model from 1b from and your final Gamma model from 2d to generate respective predictions for the number of claims in a year as well as the average amount per claim. Multiply these together and call this product purepremium_prod. Similarly, use your Tweedie model from 3b to directly predict the PurePremium value and store this in purepremium_TW.", " Chapter 11 3. Tweedie 11.1 a. (8 points) Using the entire dataset, create a profile likelihood of the index values p for use in a glm utilizing the Tweedie distribution to predict PurePremium directly. Make a plot of the profile likelihood and select the best value. (7520 - 3 points): For graduate students, I expect some exploration to find the best value as these data are quite poorly behaved. Look at the Value section of ?tweedie.profile for some hints, and I would suggest narrowing the search space for p and evaluating it on a somewhat fine grid. library(tweedie) ?tweedie.profile ## starting httpd help server ... done #per tweedie documentation on p.vec. if there is 0, below is the recommendated setting. TW_p_log &lt;- tweedie.profile(AvgClaimAmount ~ ., p.vec = seq(from=1.2,to=1.8,by=0.1), link.power = 0, do.plot = TRUE,# log link data=Insurance) ## 1.2 1.3 1.4 1.5 1.6 1.7 1.8 ## .......Done. TW_p_log &lt;- tweedie.profile(AvgClaimAmount ~ ., p.vec = seq(from=1.4,to=1.7,by=0.05), link.power = 0, do.plot = TRUE,# log link data=Insurance) ## 1.4 1.45 1.5 1.55 1.6 1.65 1.7 ## .......Done. Tweedie_p_log = TW_p_log$p.max Tweedie_p_log ## [1] 1.522449 The best value is 1.52. 11.2 b. Fit the Tweedie GLM model using the optimal power value you computed from the previous question. The file Insurance_test.csv contains a few additional observations which were not a portion of the data used to fit the previous models. We can read it in and format it using syntax similar to when we started. NewDataPoints &lt;- read.csv(./Insurance_test.csv) NewDataPoints$VehBrand &lt;- as.factor(NewDataPoints$VehBrand) NewDataPoints$VehPower &lt;- as.factor(NewDataPoints$VehPower) NewDataPoints$VehGas &lt;- as.factor(NewDataPoints$VehGas) NewDataPoints$Region &lt;- as.factor(NewDataPoints$Region) NewDataPoints$Area &lt;- as.factor(NewDataPoints$Area) NewDataPoints &lt;- read.csv(&quot;E:/Cloud/OneDrive - University of Missouri/Mizzou_PhD/Class plan/Applied Stats Model II/HW2/Insurance_test.csv&quot;) NewDataPoints$VehBrand &lt;- as.factor(NewDataPoints$VehBrand) NewDataPoints$VehPower &lt;- as.factor(NewDataPoints$VehPower) NewDataPoints$VehGas &lt;- as.factor(NewDataPoints$VehGas) NewDataPoints$Region &lt;- as.factor(NewDataPoints$Region) NewDataPoints$Area &lt;- as.factor(NewDataPoints$Area) library(statmod) # For tweedie family in glm() clot_Tweedie_log &lt;- glm(AvgClaimAmount ~ ., data=Insurance, family=tweedie(var.power=TW_p_log$p.max,link.power=0)) summary(clot_Tweedie_log) ## ## Call: ## glm(formula = AvgClaimAmount ~ ., family = tweedie(var.power = TW_p_log$p.max, ## link.power = 0), data = Insurance) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -143.413 -2.577 -2.358 -2.138 88.120 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.636e+00 2.401e-01 -6.814 9.62e-12 *** ## ClaimNb 5.551e+00 2.373e-02 233.891 &lt; 2e-16 *** ## Exposure -4.523e-01 6.460e-02 -7.002 2.56e-12 *** ## AreaB -6.157e-01 9.202e-02 -6.691 2.25e-11 *** ## AreaC -9.969e-01 1.217e-01 -8.193 2.62e-16 *** ## AreaD -1.432e+00 1.886e-01 -7.592 3.21e-14 *** ## AreaE -1.863e+00 2.469e-01 -7.547 4.54e-14 *** ## AreaF -2.206e+00 3.625e-01 -6.086 1.17e-09 *** ## VehPower5 1.935e-01 7.206e-02 2.685 0.007252 ** ## VehPower6 1.713e-01 6.842e-02 2.504 0.012298 * ## VehPower7 -1.871e-01 6.894e-02 -2.713 0.006662 ** ## VehPower8 -1.024e-01 1.057e-01 -0.968 0.332876 ## VehPower9 3.152e-01 1.066e-01 2.958 0.003098 ** ## VehPower10 4.430e-01 1.133e-01 3.910 9.23e-05 *** ## VehPower11 7.907e-01 1.423e-01 5.556 2.78e-08 *** ## VehPower12 6.662e-01 2.117e-01 3.146 0.001654 ** ## VehPower13 1.599e-01 2.723e-01 0.587 0.557015 ## VehPower14 -2.349e+00 6.682e-01 -3.515 0.000441 *** ## VehPower15 9.716e-01 4.146e-01 2.344 0.019100 * ## VehAge -3.288e-03 4.003e-03 -0.821 0.411487 ## DrivAge 1.230e-02 1.488e-03 8.271 &lt; 2e-16 *** ## BonusMalus 7.315e-03 1.185e-03 6.171 6.84e-10 *** ## VehBrandB10 7.370e-02 1.421e-01 0.519 0.603970 ## VehBrandB11 7.272e-01 1.379e-01 5.275 1.33e-07 *** ## VehBrandB12 -4.509e-01 9.113e-02 -4.948 7.54e-07 *** ## VehBrandB13 1.093e+00 1.305e-01 8.376 &lt; 2e-16 *** ## VehBrandB14 6.719e-01 2.192e-01 3.064 0.002182 ** ## VehBrandB2 9.120e-02 5.343e-02 1.707 0.087828 . ## VehBrandB3 -7.350e-02 7.786e-02 -0.944 0.345144 ## VehBrandB4 3.675e-01 1.017e-01 3.614 0.000302 *** ## VehBrandB5 4.190e-01 8.288e-02 5.055 4.33e-07 *** ## VehBrandB6 3.926e-01 8.891e-02 4.416 1.01e-05 *** ## VehGasRegular -2.223e-01 4.420e-02 -5.028 4.97e-07 *** ## Density 3.520e-01 4.718e-02 7.460 8.80e-14 *** ## RegionR21 6.821e-01 4.580e-01 1.489 0.136462 ## RegionR22 3.816e-01 2.036e-01 1.875 0.060840 . ## RegionR23 6.810e-01 3.349e-01 2.034 0.041990 * ## RegionR24 7.622e-01 1.072e-01 7.107 1.20e-12 *** ## RegionR25 2.417e-02 1.975e-01 0.122 0.902627 ## RegionR26 5.522e-01 2.320e-01 2.380 0.017305 * ## RegionR31 8.848e-01 1.478e-01 5.986 2.16e-09 *** ## RegionR41 -3.213e-02 1.409e-01 -0.228 0.819610 ## RegionR42 7.103e-02 2.552e-01 0.278 0.780753 ## RegionR43 1.662e-01 7.882e-01 0.211 0.832964 ## RegionR52 4.081e-01 1.255e-01 3.250 0.001153 ** ## RegionR53 7.129e-01 1.176e-01 6.061 1.36e-09 *** ## RegionR54 -7.083e-02 1.370e-01 -0.517 0.605221 ## RegionR72 5.431e-02 1.322e-01 0.411 0.681283 ## RegionR73 6.923e-01 2.178e-01 3.179 0.001479 ** ## RegionR74 -6.384e-01 1.781e-01 -3.585 0.000337 *** ## RegionR82 6.612e-01 1.075e-01 6.150 7.83e-10 *** ## RegionR83 -8.528e-02 4.404e-01 -0.194 0.846456 ## RegionR91 5.483e-01 1.659e-01 3.305 0.000949 *** ## RegionR93 8.103e-01 1.147e-01 7.065 1.63e-12 *** ## RegionR94 -2.268e-01 5.804e-01 -0.391 0.695913 ## PurePremium 3.061e-07 1.992e-08 15.364 &lt; 2e-16 *** ## Frequency 2.106e-02 1.861e-03 11.320 &lt; 2e-16 *** ## Predicted_Class -1.096e+00 4.752e-01 -2.307 0.021053 * ## Predicted_Class_NB NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Tweedie family taken to be 52.21709) ## ## Null deviance: 2712009 on 44998 degrees of freedom ## Residual deviance: 620930 on 44941 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 20 11.3 c.(7 points) Use your Poisson model from 1b from and your final Gamma model from 2d to generate respective predictions for the number of claims in a year as well as the average amount per claim. Multiply these together and call this product purepremium_prod. Similarly, use your Tweedie model from 3b to directly predict the PurePremium value and store this in purepremium_TW. count(Insurance, Predicted_Class) ## Predicted_Class n ## 1 0 44994 ## 2 1 4 ## 3 2 1 amt_predict = predict(Insurance_GammaGLM) mean(amt_predict) ## [1] 7.360522 library(dplyr) count(Insurance, Predicted_Class) ## Predicted_Class n ## 1 0 44994 ## 2 1 4 ## 3 2 1 purepremium_prod = 44999 * mean(Total_Prob0) * mean(amt_predict) #using the ZIP model. total # of insurances * average claims predicted * average claim amount predicted. #??? won&#39;t it just be the average of the original dataset? purepremium_prod ## [1] 314243.7 The purepremium_prod will be $314,268.8 using ZIP model clot_Tweedie_log_predict &lt;- predict(clot_Tweedie_log,data = &#39;Insurance&#39;, type=&quot;response&quot;) p_MLE &lt;- TW_p_log$p.max phi_MLE &lt;- TW_p_log$phi.max #Prob_Zero_Model &lt;- exp(-mu.Phases(2 - p_MLE)/(phi_MLE*(2 - p_MLE))) #error unexpected input??? what does it mean? #S46: CPoiGam &lt;- tweedie.convert(xi=p_MLE, mu=clot_Tweedie_log_predict, phi=phi_MLE) PoiGamPred &lt;- rbind(&quot;Poisson mean&quot; = CPoiGam$poisson.lambda, &quot;Gamma mean&quot; = CPoiGam$gamma.mean, &quot;Gamma dispersion&quot; = CPoiGam$gamma.phi) mean(CPoiGam$poisson.lambda * CPoiGam$gamma.mean) ## [1] 32.47151 purepremium_TW = 44999 * 32 purepremium_TW ## [1] 1439968 The purepremium_TW will be $1,439,968 using Tweedie model #??? why the two results are so different? "],["stat-45207520---homework-3-spring-2022-due-march-30-2022.html", "Chapter 12 STAT 4520/7520 - Homework 3 Spring 2022 Due: March 30, 2022 12.1 1) The cake dataset in the lme4 package contains data on an experiment was conducted to determine the effect of recipe and baking temperature on chocolate cake quality. Fifteen batches of cake mix for each recipe were prepared. Each batch was sufficient for six cakes. Each of the six cakes was baked at a different temperature which was randomly assigned. As an indicator of quality, the breakage angle of the cake was recorded. 12.2 2) The purpose of random effects is to remove variation from known sources. In this problem, we will use this idea and see how it can work with a popular technique for predictive modeling, the random forest. Random forests were covered in the prerequisite course, and can be fit using the randomForest function in the randomForest library. The dataset Problem-2.csv has a simulated dataset. There is a nonlinear relationship between 6 numeric explanatory variables x1, . . . x6 and the response y, to addition to a grouping effect according to the ID variable, which has 100 levels.", " Chapter 12 STAT 4520/7520 - Homework 3 Spring 2022 Due: March 30, 2022 12.1 1) The cake dataset in the lme4 package contains data on an experiment was conducted to determine the effect of recipe and baking temperature on chocolate cake quality. Fifteen batches of cake mix for each recipe were prepared. Each batch was sufficient for six cakes. Each of the six cakes was baked at a different temperature which was randomly assigned. As an indicator of quality, the breakage angle of the cake was recorded. Note: temperate randomly assigned. receipt, batches/replicate are nested in receipt!!! It contains the variables:  replicate: batch number from 1 to 15  recipe: 3 Recipes, A, B, and C  temperature: temperature at which cake was baked: 175C 185C 195C 205C 215C 225C  angle: a numeric vector giving the angle at which the cake broke.  temp: numeric value of the baking temperature (degrees F). The data can be loaded with the command. data(cake,package = &quot;lme4&quot;) Additionally, we will only use the numeric version of temperature. #library(dplyr) #library(tidyverse) #??? Why negative before the temperature. what does it mean? cake &lt;- cake |&gt; dplyr::select(-temperature) 12.1.1 a. Of the explanatory variables recipe, temp, and replicate, which are fixed and which are random? Explain. Is there any nesting in these variables? The recipe and Temp are fixed, since I am genuinely interested in their different levels. Replicate are random, since it is random sample of levels. I dont care about the different replications. The replicate/batch is nested within the recipe. # random affect has nothing to do with random assignment in actual??? # S39: egg example, is the sample nested within technician? further split GH into G1 G2? not sure what it means. How about here is the temperature also nested??? when is it not nested? the sample are distributed across technician??? or technician switch between labs? 12.1.2 b. Fit a linear model with no random effects for breaking angle against the interaction between recipe and temperature. Which variables are significant? What is the t value for temp the temperature variable? What is the RMSE of this model? lm_model &lt;- lm(angle ~ recipe*temp, cake) summary(lm_model) ## ## Call: ## lm(formula = angle ~ recipe * temp, data = cake) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.759 -5.279 -1.448 3.019 27.505 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.379365 9.656850 0.246 0.80557 ## recipeB -3.649206 13.656848 -0.267 0.78952 ## recipeC -1.941270 13.656848 -0.142 0.88707 ## temp 0.153714 0.048109 3.195 0.00157 ** ## recipeB:temp 0.010857 0.068037 0.160 0.87334 ## recipeC:temp 0.002095 0.068037 0.031 0.97546 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.795 on 264 degrees of freedom ## Multiple R-squared: 0.1159, Adjusted R-squared: 0.0992 ## F-statistic: 6.925 on 5 and 264 DF, p-value: 4.285e-06 anova(lm_model) ## Analysis of Variance Table ## ## Response: angle ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## recipe 2 135.1 67.54 1.1117 0.3305 ## temp 1 1966.7 1966.71 32.3709 3.377e-08 *** ## recipe:temp 2 1.7 0.87 0.0143 0.9858 ## Residuals 264 16039.4 60.76 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The temperature variable is significant. The t value is 3.195 The RMSE is 7.79487, the MSE is 60.76 sqrt(60.76) ## [1] 7.79487 12.1.3 c. Fit a mixed model to the data which takes into account the replicates of the recipes. Examine the variance components. Is there variation in the batches made with each recipe? How does the Residual variance component compare to the MSE from (b)? What is the t-value for temperature? # Test non-nested structure. #library(lme4) #mixed_model &lt;- lmer(angle ~ recipe*temp + (1 | replicate), cake) #summary(mixed_model) #anova(mixed_model) # Test the nested structure library(lme4) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## ## Attaching package: &#39;lme4&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## cake mixed_model &lt;- lmer(angle ~ recipe*temp + (1 | recipe) + (1 | recipe:replicate), cake) summary(mixed_model) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: angle ~ recipe * temp + (1 | recipe) + (1 | recipe:replicate) ## Data: cake ## ## REML criterion at convergence: 1703.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.4599 -0.5909 -0.0967 0.6103 2.6578 ## ## Random effects: ## Groups Name Variance Std.Dev. ## recipe:replicate (Intercept) 41.7674 6.4628 ## recipe (Intercept) 0.5087 0.7132 ## Residual 20.8862 4.5701 ## Number of obs: 270, groups: recipe:replicate, 45; recipe, 3 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 2.379365 5.945735 0.400 ## recipeB -3.649206 8.408539 -0.434 ## recipeC -1.941270 8.408539 -0.231 ## temp 0.153714 0.028208 5.449 ## recipeB:temp 0.010857 0.039891 0.272 ## recipeC:temp 0.002095 0.039891 0.053 ## ## Correlation of Fixed Effects: ## (Intr) recipB recipC temp rcpB:t ## recipeB -0.707 ## recipeC -0.707 0.500 ## temp -0.949 0.671 0.671 ## recipeB:tmp 0.671 -0.949 -0.474 -0.707 ## recipeC:tmp 0.671 -0.474 -0.949 -0.707 0.500 anova(mixed_model) ## Analysis of Variance Table ## npar Sum Sq Mean Sq F value ## recipe 2 8.89 4.45 0.2129 ## temp 1 1966.71 1966.71 94.1631 ## recipe:temp 2 1.74 0.87 0.0417 Yes. There is variation in the batches made with each receipt.Since a significant portion of variance comes from the replicate (i.e., 66%) as below. 41.7674 /(41.7674 +20.8862 + 0.5087) ## [1] 0.661271 The residual variance component is smaller now with 20 compared with 60 before. The t value for hte temperature is 5.4 or more significant than before. # ??? why replicate not receipt? as the levels? ??? why ANOVA table not match the random effect table? 12.1.4 d. Test for a recipe and temperature effect. library(MCMCglmm) ## Loading required package: coda ## Loading required package: ape summary(MCMCglmm(angle ~ recipe*temp,random=~replicate, data=cake, verbose=FALSE))$solution ## post.mean l-95% CI u-95% CI eff.samp pMCMC ## (Intercept) 2.144956864 -8.77697344 15.19327939 1000 0.718 ## recipeB -3.325454537 -17.45415797 14.74041624 1000 0.740 ## recipeC -2.040549300 -19.46512878 11.36814295 1000 0.828 ## temp 0.154883860 0.10071669 0.21322479 1000 0.001 ## recipeB:temp 0.009252442 -0.06566437 0.09287612 1000 0.856 ## recipeC:temp 0.002601202 -0.06744861 0.08693204 1000 0.960 #??? how to specific the random structure here? There is no simple or interaction effect for recipe. The temperature is siginificant at 99% confidence level with pMCMC at 0.001. 12.1.5 e. Create a QQ plot of the residuals and a residual vs predicted plot for the linear model in (b) and the mixed model in (d), describe how well the model assumptions are satisfied for each. qqnorm(residuals(lm_model),main=&quot;QQ plot of Residuals - Linear model&quot;) qqline(residuals(lm_model)) qqnorm(residuals(mixed_model),main=&quot;QQ plot of Residuals - Mixed model&quot;) qqline(residuals(mixed_model)) The mixed model satisfied the normality assumption much better as the points are closer to the line on the QQ Plot. 12.1.6 f. (7510 only) Examine the BLUPs from the mixed model in (d). Do you notice any patterns? library(lme4) ranef(mixed_model)$replicate ## NULL ranef(mixed_model)$recipe ## (Intercept) ## A -3.077238e-14 ## B -4.667144e-14 ## C -2.051492e-14 The number seems to be close to 0. #??? what patten central point?variance or intercept? #??? why replicate null? #!!! Alternative solution: with nested structure for HW3. replication is nested within recipe.??? 12.2 2) The purpose of random effects is to remove variation from known sources. In this problem, we will use this idea and see how it can work with a popular technique for predictive modeling, the random forest. Random forests were covered in the prerequisite course, and can be fit using the randomForest function in the randomForest library. The dataset Problem-2.csv has a simulated dataset. There is a nonlinear relationship between 6 numeric explanatory variables x1, . . . x6 and the response y, to addition to a grouping effect according to the ID variable, which has 100 levels. set.seed(1) Problem2 &lt;- read.csv(&quot;E:/Cloud/OneDrive - University of Missouri/Mizzou_PhD/Class plan/Applied Stats Model II/HW3/Problem-2.csv&quot;, stringsAsFactors = TRUE) 12.2.1 a. Read in the dataset, and split the data into a training and test set, using 95% of the data for the training set. Set a seed of 1 for consistency of results. train=sample(1:nrow(Problem2), nrow(Problem2)*0.95) Problem2.train = Problem2[train,] Problem2.test = Problem2[-train,] 12.2.2 b. Fit a random forest model to the training data using y as the response with x1, . . . x6 and ID as the explanatory variables. Use the model to predict y in the test set and compute the test root mean squared error ( RMSE ). #install.packages(&quot;randomForest&quot;) library(randomForest) ## randomForest 4.7-1 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin RF_model =randomForest(y~x1+x2+x3+x4+x5+x6+ID,data=Problem2,subset=train,importance=TRUE) Problem2.test$y_RF &lt;- predict(RF_model,newdata=Problem2.test) sqrt(sum((Problem2.test$y_RF - Problem2.test$y)^2)/175) ## [1] 4.503223 The RMSE is 4.5 12.2.3 c. Fit the mixed model yij = µ + IDi + ij to the training data. This model does not use the x_i at all, and has a random intercept based on ID. After fitting the model, extract the BLUPs for ID and add them to the constant µ, the estimate of the overall intercept. What do these values represent? Store them in a data frame along with a column storing the IDs. P2_mixed_model &lt;- lmer(y ~ 1 + (1 | ID), Problem2.train) summary(P2_mixed_model) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: y ~ 1 + (1 | ID) ## Data: Problem2.train ## ## REML criterion at convergence: 21249.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.9799 -0.6991 -0.0011 0.7052 3.2476 ## ## Random effects: ## Groups Name Variance Std.Dev. ## ID (Intercept) 7.018 2.649 ## Residual 32.786 5.726 ## Number of obs: 3325, groups: ID, 100 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 24.5945 0.2829 86.92 #P2_ID_BLUP = ranef(P2_mixed_model)$ID library(tidyverse) #S23: #predict(mixed_model, newdata=data.frame(operator=&quot;a&quot;)) Problem2.test$BLUP &lt;- predict(P2_mixed_model, newdata=Problem2.test) DF_BLUPx &lt;- fixef(P2_mixed_model) + ranef(P2_mixed_model)$ID DF_BLUPx$ID &lt;- 1:nrow(DF_BLUPx) The values represent the intercept for each ID. 12.2.4 d. Add a column to the test dataset containing the values obtained in part c, joined to the test dataset by matching the IDs. Note: with the dplyr package loaded, you can quickly join by ID using a line like: Data.Test &lt;- left_join(Data.Test, int_plus_blups, by = ID) Examine the first few rows and describe what the new column represents. Problem2.test &lt;- left_join(Problem2.test, DF_BLUPx, by = &quot;ID&quot;) The new column represents the intercept for each ID in the test dataset. 12.2.5 e. Compute the residuals of the mixed model fit in (c) using the residuals() function, store them in a column of the training data. Problem2.train$residualx = residuals(P2_mixed_model) 12.2.6 f. Fit another random forest model to the training data, using the residuals from part (e) as the response and only x1, . . . x6 as the explanatory variables. Use this model to predict into the test data and store these predictions. What do these predictions represent? #why I cannot use subset anymore??? diff dataaset? RF_residuals_model =randomForest(residualx~x1+x2+x3+x4+x5+x6,data=Problem2.train,importance=TRUE) Problem2.test$y_RF_residual &lt;- predict(RF_residuals_model,newdata=Problem2.test) The prediction represents the additional variations beyond the IDs variation. 12.2.7 g. Add the predictions in (f) to the values joined to the test data in (d). Consider these values test predictions, and compute the test MSE. Compare this to the test MSE compared in part (b). What do you observe? sqrt(sum((Problem2.test$BLUP + Problem2.test$y_RF_residual - Problem2.test$y)^2)/175) ## [1] 3.608027 (3.6-4.5)/4.5 ## [1] -0.2 The RMSE is reduced from 4.5 to 3.6 or a 20% reduction. The models prediction is greatly improved. "],["applied-stats-model-ii-lecture-note.html", "Chapter 13 Applied Stats Model II Lecture Note 13.1 List of Questions: 13.2 Class content overview: 13.3 GLM-CategoricalData_SP2022 13.4 GLM-CountData_SP2022 13.5 GLMs_AdvancedTopics_SP2022 13.6 MM-RandomEffects_Spring2022", " Chapter 13 Applied Stats Model II Lecture Note 13.1 List of Questions: 13.1.1 HW1 13.1.2 HW2 2b: bin it? 13.1.3 Midterm Q1: ??? #the intercept is 3.5 #??? does the intercept needs the log? GammaPoisson - why contains no 0s? -&gt; only probability. unless I set a cut-off point. ICC random variance/ total variance. why specify the mean = mean() in summarise. duplicate &gt; creating a variable name Q7c: # Convert back to glm() style object ??? why needs a conversion? # why alternative style reached? # is it the aggregated model vs. individual count? or the same??? Q7a # then why the 70 and 60 combination has service??? Q7f: #??? why is the actual data 10% 0, but here is 4.5%? Diff so much 13.2 Class content overview: Textbook: Extending the Linear Model with R, Second Edition, 2016 by Julian Faraway Topics:  Generalized Linear Models  Models for Proportional data.  Models for Count data.  Models for Positive continuous data.  Mixed Models  Random Effects  Linear Mixed Models  Generalized Linear Mixed Models  Repeated Measures  Longitudinal Data  Dependent Data  Spatial Dependence.  Spatial Point Patterns.  Time Series.  Nonlinear Models  Neural Networks  Deep Learning 13.3 GLM-CategoricalData_SP2022 13.4 GLM-CountData_SP2022 13.5 GLMs_AdvancedTopics_SP2022 data(quilpie,package = &quot;GLMsData&quot;) quilpie$Phase &lt;- as.factor(quilpie$Phase) #head(quilpie,3) |&gt; DisplayTable library(tweedie) Rain_TW_P &lt;- tweedie.profile(Rain ~ Phase, p.vec = seq(from=1,to=3,by=0.01), do.plot=TRUE, data=quilpie) ## Values of p between 0 and 1 and less than zero have been removed: such values are not possible. ## When the response variable contains exact zeros, all values of p must be between 1 and 2; other values have been removed. ## 1.01 1.02 1.03 1.04 1.05 1.06 1.07 1.08 1.09 1.1 1.11 1.12 1.13 1.14 1.15 1.16 1.17 1.18 1.19 1.2 1.21 1.22 1.23 1.24 1.25 1.26 1.27 1.28 1.29 1.3 1.31 1.32 1.33 1.34 1.35 1.36 1.37 1.38 1.39 1.4 1.41 1.42 1.43 1.44 1.45 1.46 1.47 1.48 1.49 1.5 1.51 1.52 1.53 1.54 1.55 1.56 1.57 1.58 1.59 1.6 1.61 1.62 1.63 1.64 1.65 1.66 1.67 1.68 1.69 1.7 1.71 1.72 1.73 1.74 1.75 1.76 1.77 1.78 1.79 1.8 1.81 1.82 1.83 1.84 1.85 1.86 1.87 1.88 1.89 1.9 1.91 1.92 1.93 1.94 1.95 1.96 1.97 1.98 1.99 ## ...................................................................................................Done. rain_Tweedie &lt;- glm(Rain ~ Phase, data=quilpie, family=tweedie(var.power=Rain_TW_P$p.max, link.power=0)) coef(summary(rain_Tweedie)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.169054 1.956012 -1.108916 0.271681607 ## Phase2 5.692284 1.967831 2.892670 0.005238606 ## Phase3 3.515270 2.060019 1.706426 0.092854120 ## Phase4 5.026875 1.972906 2.547954 0.013287172 ## Phase5 4.646792 1.973386 2.354730 0.021665216 Phases &lt;- data.frame(Phase = factor(c(1, 2, 3, 4, 5))) mu.Phases &lt;- predict(rain_Tweedie, newdata=Phases,type=&quot;response&quot;) p_MLE &lt;- Rain_TW_P$p.max #tweedie profile phi_MLE &lt;- Rain_TW_P$phi.max #phi? -&gt; dispersion para Prob_Zero_Model_orig &lt;- exp(-mu.Phases^(2 - p_MLE)/(phi_MLE*(2 - p_MLE))) # probability of it being 0. prob of observe rain + how much rain. #??? quilpie %&gt;% group_by(Phase) %&gt;% summarize(prop0_dat = mean(Rain==0)) %&gt;% cbind(Prob_Zero_Model) summary(Prob_Zero_Model_orig) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.07252 0.17811 0.25719 0.39032 0.51390 0.92989 Phases &lt;- data.frame(Phase = factor(c(1, 2, 3, 4, 5))) mu.Phases &lt;- predict(rain_Tweedie, newdata=Phases,type=&quot;response&quot;) #some contribution from possion + gamma, always positive. p_MLE &lt;- Rain_TW_P$p.max phi_MLE &lt;- Rain_TW_P$phi.max Prob_Zero_Model &lt;- exp(-mu.Phases^(2 - p_MLE)/(phi_MLE*(2 - p_MLE))) library(tidyverse) #quilpie %&gt;% group_by(Phase) %&gt;% summarize(prop0_dat = mean(Rain==0)) %&gt;% cbind(Prob_Zero_Model) summary(mu.Phases) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.1143 3.8429 11.9143 13.4377 17.4235 33.8937 summary(Prob_Zero_Model) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.07252 0.17811 0.25719 0.39032 0.51390 0.92989 library(tidyverse) #package of packages. needed for the |&gt; , groupby, summarise need tidyverse. quilpie |&gt; group_by(Phase) |&gt; summarize(prop0_dat = mean(Rain==0)) |&gt; cbind(Prob_Zero_Model) ## Phase prop0_dat Prob_Zero_Model ## 1 1 0.8571429 0.92988539 ## 2 2 0.0000000 0.07252326 ## 3 3 0.7142857 0.51390033 ## 4 4 0.2352941 0.17810940 ## 5 5 0.2380952 0.25718557 #prop0_dat: what % has , actual vs. predicted. quilpie&#39;s variable contains the Rain. # summarise(mean = mean()) #86 count = a name CPoiGam &lt;- tweedie.convert(xi=p_MLE, mu=mu.Phases, phi=phi_MLE) PoiGamPred &lt;- rbind(&quot;Poisson mean&quot; = CPoiGam$poisson.lambda, &quot;Gamma mean&quot; = CPoiGam$gamma.mean, &quot;Gamma dispersion&quot; = CPoiGam$gamma.phi) colnames(PoiGamPred) &lt;- paste0(&quot;Phase&quot;, 1:5) #PoiGamPred %&gt;% round(4) %&gt;% DisplayTable #??? the %&gt;% = |&gt; but not work? #PoiGamPred |&gt; round(4) |&gt; DisplayTable #https://stackoverflow.com/questions/67744604/what-does-pipe-greater-than-mean-in-r PoiGamPred ## Phase1 Phase2 Phase3 Phase4 Phase5 ## Poisson mean 0.07269394 2.623848 0.6657259 1.725357 1.3579574 ## Gamma mean 0.16582834 1.362530 0.6088689 1.065178 0.9254371 ## Gamma dispersion 1.45160559 97.999331 19.5694557 59.892898 45.2090019 #??? no exact 0s? 13.6 MM-RandomEffects_Spring2022 random effect: not interested in specific level. vs. fixed effect - not individual slope? dummy interaction with the focal variable. fixed effect test the mean vs. random effect test the variance (not interested in particular mean, want to see if mean actually varied across levels). ??? S11: variance depends on the random effect but mean is not affected? P14: reading the box plot. mean, median, range??? P15: whats the n? Whats the Sum Sq? P21: shift in the mean vs. variance and mean 0. how do I know how much the mean is shifted??? data(pulp, package=&quot;faraway&quot;) plot(y=pulp$bright, x=pulp$operator,xlab = &quot;Operator&quot;, ylab = &quot;Brightness&quot;) lm_model &lt;- lm(bright ~ operator, pulp) anova(lm_model) ## Analysis of Variance Table ## ## Response: bright ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## operator 3 1.34 0.44667 4.2039 0.02261 * ## Residuals 16 1.70 0.10625 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Prediction for each operator is simply the sum of fixed and random effect = BLUP??? components. -&gt; S24: not in the data. new operator will take a 0 in intercept change? but the intercept can be predicted with the Xs??? -&gt; yes, if there is blocking. how is the blend different from the operator as blocking? discrete vs. continuous? S30: how does it match? shouldnt it be mean + the BLUPS??? S34: fixed are irrigation that is interest. field randomly = replications? Blocking Split plot Nested effect vs. random effect? Residual plots General: interaction terms = depends. must happen when sth happen then interaction. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
