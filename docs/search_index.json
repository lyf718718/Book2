[["applied-stats-model-ii---exam-two-stat-45207520---exam-2.html", "Chapter 7 Applied Stats Model II - Exam Two STAT 4520/7520 - Exam 2 7.1 1. (7 points) Consider the two plots below, which show the exact same (x, y) data. The difference is that the right plot has colored points based on a grouping variable. Is there a relationship between x and y? If not, how can we explain the left plot? Finally, how would you model the response y? 7.2 2. (3 points) Say we have a fitted ARMA(p,q) model given by\\(Y_t = .3Y_{t−1} + ε_t\\) What are the values of p and q? 7.3 3. Say we observe 100 events in a 20ft × 10ft spatial field under a homogeneous Poisson process: 7.4 4. (6 points) Suppose we are measuring some response variable and our data contains 5 locations. How would our research goals be different if we considered the location factor as fixed vs random? 7.5 5. (7 points) Using the below keras code and summary output, justify the number of parameters at each layer of the neural network. 7.6 6. Consider the below ANN which was fit to predict a cereal’s rating based on its protein content. Using the provided weights and biases with an input of protein=4, find: 7.7 7. (7 points) Consider the following ACF plot. 7.8 8. (5 points) The file 1dConvolutionData.csv contains a collection of 100 sequential points. Read them into R and convolve them with the kernal c(0.054, 0.242, 0.389, 0.242, 0.054). Place the orginal signal as well as the output from the convolution on the same plot(be sure to line them up properly). What did this kernal do? 7.9 9. Time series (20 points) 7.10 10. Open Ended Analysis in R (29 points)", " Chapter 7 Applied Stats Model II - Exam Two STAT 4520/7520 - Exam 2 7.1 1. (7 points) Consider the two plots below, which show the exact same (x, y) data. The difference is that the right plot has colored points based on a grouping variable. Is there a relationship between x and y? If not, how can we explain the left plot? Finally, how would you model the response y? No relationship between X and Y if I control for the grouping variable. Within each group, x and y has no relationship. I will build a model of Y = X + four dummy variables (5-1 categories). 7.2 2. (3 points) Say we have a fitted ARMA(p,q) model given by\\(Y_t = .3Y_{t−1} + ε_t\\) What are the values of p and q? p is 1 q is 0 7.3 3. Say we observe 100 events in a 20ft × 10ft spatial field under a homogeneous Poisson process: • (4 points) What is the estimate of λ? \\(\\lambda\\) is 100 or the average events • (3 points) If we considered a 100ft × 100ft region, how many events would we expect? 100 * 100 / (20 * 10) ## [1] 50 Since the size is 50 times larger and is a homogeneous poisson process, we will expect 50 * 100 = 5000 events. 7.4 4. (6 points) Suppose we are measuring some response variable and our data contains 5 locations. How would our research goals be different if we considered the location factor as fixed vs random? The location is considered fixed if we are interested in the specific 5 locations’ effect. The location is considered as random if we are NOT interested in their specific effect. We consider the five locations as a random sample of n locations. 7.5 5. (7 points) Using the below keras code and summary output, justify the number of parameters at each layer of the neural network. library(keras) model &lt;- keras_model_sequential() |&gt; layer_dense(units=8, activation= &quot;relu&quot;, input_shape=1) |&gt; layer_dense(units=4, activation= &quot;relu&quot;) |&gt; layer_dense(units=1, activation= &quot;linear&quot;) ## Loaded Tensorflow version 2.8.0 model |&gt; summary() ## Model: &quot;sequential&quot; ## ________________________________________________________________________________ ## Layer (type) Output Shape Param # ## ================================================================================ ## dense_2 (Dense) (None, 8) 16 ## ## dense_1 (Dense) (None, 4) 36 ## ## dense (Dense) (None, 1) 5 ## ## ================================================================================ ## Total params: 57 ## Trainable params: 57 ## Non-trainable params: 0 ## ________________________________________________________________________________ There are units with bias for each layers.Thus, First layer: 1 input (input_Shape =1) with one bias for each hidden unit. 2 * 8 = 16 parameters. Second layer: 8 units + 1 bias unit = 9 input units for each hidden unit (4 hidden units in the layer). 9 * 4 = 36 parameters. Third layer: 4 units + 1 bias unit = 5 input units for each final output unit (1 output unit. 5*1 = 5 parameters. 7.6 6. Consider the below ANN which was fit to predict a cereal’s rating based on its protein content. Using the provided weights and biases with an input of protein=4, find: (6 points) The output value of each of the 3 neurons in the hidden layer assuming sigmoid activations. x = 4 y_Sigmoid &lt;- function(x) 1/(1 + exp(-x)) h1 &lt;- y_Sigmoid((-0.85181)*x + (-0.28327)) h2 &lt;- y_Sigmoid((0.01593)*x + (1.04183)) h3 &lt;- y_Sigmoid((-0.68229)*x + (0.52639)) print(h1) ## [1] 0.02435147 print(h2) ## [1] 0.7512986 print(h3) ## [1] 0.09950202 The output values are the above. (3 points) The output rating value assuming a linear activation. Be sure to show your calculatons (feel free to use R) # linear is identity? y_identity &lt;- function(x) x o1 &lt;- y_identity((-0.80025)*0.02435147 + (1.23886)*0.7512986 + (-0.69436)*0.09950202 + (0.10131)) o1 ## [1] 0.9434863 The output rating value is 0.9434863 7.7 7. (7 points) Consider the following ACF plot. Which of the following models is more likely to have generated this plot? Explain why. Model A: \\(Y_t = 0.9Y_{t−1} + ε_t\\) Model B: \\(Y_t = −0.9Y_{t−1} + ε_t\\) Model A is more likely, as the dependency gets weaker. 7.8 8. (5 points) The file 1dConvolutionData.csv contains a collection of 100 sequential points. Read them into R and convolve them with the kernal c(0.054, 0.242, 0.389, 0.242, 0.054). Place the orginal signal as well as the output from the convolution on the same plot(be sure to line them up properly). What did this kernal do? #S4 - what does it mean to trim. how does the code work with kernal? purpose of kernal? can&#39;t we observe it already without it? not as obvious? large larger reverse is log? cnn_data &lt;- read.csv(&quot;E:/Cloud/OneDrive - University of Missouri/Mizzou_PhD/Class plan/Applied Stats Model II/Exam 2/1dConvolutionData.csv&quot;, stringsAsFactors = TRUE) input &lt;- cnn_data$x kernal &lt;- c(0.054, 0.242, 0.389, 0.242, 0.054) output &lt;- NULL for(i in 2:(length(input)-1)){ value &lt;- sum(input[(i-1):(i+1)]*kernal) output &lt;- c(output, value) } print(output) ## [1] -0.961955625 -1.915538333 -2.995930056 -3.848004043 -4.551280610 ## [6] -4.972453783 -5.033774992 -5.080213936 -4.524158304 -4.088413561 ## [11] -3.275951905 -3.102833509 -3.451367383 -4.049387298 -4.511878784 ## [16] -4.663058544 -4.761963824 -5.152595067 -4.706440185 -4.597501059 ## [21] -4.689175074 -5.577107860 -6.022871498 -6.771229669 -7.215228565 ## [26] -7.950927279 -8.239379008 -8.105257819 -8.214729089 -8.131398312 ## [31] -8.220891333 -7.794180158 -8.115268754 -7.727446213 -7.531897294 ## [36] -6.767634541 -6.670732895 -6.322696737 -5.833421186 -5.022111464 ## [41] -4.446935327 -4.030695578 -3.563526225 -3.320498284 -3.006107104 ## [46] -3.109237138 -3.221920085 -3.387963155 -3.518413372 -3.333375517 ## [51] -3.052103219 -2.857732771 -2.926663595 -3.336366490 -3.564824304 ## [56] -4.023990815 -4.305621947 -4.471155365 -4.298629347 -4.374062962 ## [61] -4.280446451 -4.473147585 -4.725700439 -4.750539800 -5.097070001 ## [66] -5.502924696 -6.155508490 -6.356331049 -6.464565527 -6.038974647 ## [71] -5.815253546 -4.991305976 -4.595765064 -3.785075493 -3.588631575 ## [76] -2.613430490 -2.191058762 -1.328527541 -0.733745698 0.208060687 ## [81] 0.785040471 1.145025209 0.826156353 0.012339936 -0.304906599 ## [86] 0.043322054 0.437272676 0.508118142 -0.336632956 -0.423463117 ## [91] -0.470591518 -0.114943473 -0.501087652 -0.364161922 -0.417163670 ## [96] 0.002408011 -0.008509076 0.478793847 #plot(cnn_data$x[2:99],output,type=&quot;hist&quot;,col= c(&quot;blue&quot;,&quot;red&quot;)) par(mfrow = c(1,2)) hist(output) hist(cnn_data$x[2:99]) The kernal help me find the peak. since the middle points are larger than the surrounding points. 7.9 9. Time series (20 points) An AR(1) model with φ = 1 is often referred to as a random walk model. #??? AR(1) vs. theta = 1, S17 7.9.1 a. (3 points) Write the model for Yt mathematically and explain the intuition behind this naming convention. \\(Y_t = Y_{t−1} + ε_t\\) next period is the same distribution and independent from the previous period plus some random error. It is like walk in a random direction. 7.9.2 b. (6 points) Using a random error process of ε ∼ N(0, 1), start a random walk from \\(Y_1 = 0\\) and continue for 1000 steps. Plot the time series. set.seed(1); n &lt;- 1000; eps_t &lt;- rnorm(n, sd = 1) Y1 &lt;- Y2 &lt;- Y3 &lt;- double(n) Y1[1] &lt;- Y2[1] &lt;- Y3[1] &lt;- 0 # Y_t = phi * Y_(t-1) + eps_t for(i in 2:n) Y1[i] &lt;- 1 * Y1[i-1] + eps_t[i] plot.ts(Y1,ylim = c(-100,100)); abline(h=0,lty=2) #S24 dependent ppt ??? how to set it as AR(1)? theta? #set.seed(1); n &lt;- 1000; eps_t &lt;- rnorm(n, sd = 1); par(mfrow=c(1,1)) #AR &lt;- MA &lt;- ARMA &lt;- double(n) #AR[1:2] &lt;- MA[1:2] &lt;- ARMA[1:2] &lt;- 0 #for(i in 3:n) AR[i] &lt;- sum(c(.6, .2) * AR[(i-1):(i-2)]) + eps_t[i] 7.9.3 c. (6 points) Provide both an autocorrelation plot as well as a partial autocorrelation plot. Explain why they are so different. #S12 ??? why non-constant variance? #S18 lm_Y1 &lt;- lm( Y1 ~ I(1:n)) par(mfrow = c(1,2)) acf(resid(lm_Y1)) pacf(resid(lm_Y1)) Partial ACF takes out the general autocorrelation trend while ACF does not (i.e., preseve all the autocorrleation). 7.9.4 d. (5 points) Change the value of φ to be something larger than 1 and generate another time series. Plot the result and on comment on what you observe. Why does the AR(1) model have the restriction that |φ| &lt; 1? set.seed(1); n &lt;- 1000; eps_t &lt;- rnorm(n, sd = 1) Y1 &lt;- Y2 &lt;- Y3 &lt;- double(n) Y1[1] &lt;- Y2[1] &lt;- Y3[1] &lt;- 0 # Y_t = phi * Y_(t-1) + eps_t for(i in 2:n) Y2[i] &lt;- 1.2 * Y2[i-1] + eps_t[i] plot.ts(Y2,ylim = c(-100,100)); abline(h=0,lty=2) The model explodes. The Y2 goes to infinite. AR(1) must have the restriction to keep it from exploding. 7.10 10. Open Ended Analysis in R (29 points) The data given in salamanders.csv contains counts of salamanders with site and sampling covariates. Each of 23 sites was sampled 4 times. The variables are as follows: • site: name of a location where repeated samples were taken • mined: factor indicating whether the site was affected by mountain top removal coal mining • sample: repeated sample • spp: abbreviated species name, possibly also life stage • count: number of salamanders observed We are primarily interested if mining is playing an impact on the counts of salamanders, and also if this effect depends on the species of salamander. salamanders &lt;- read.csv(&quot;E:/Cloud/OneDrive - University of Missouri/Mizzou_PhD/Class plan/Applied Stats Model II/Exam 2/salamanders.csv&quot;, stringsAsFactors = TRUE) (4 points) Examine the distribution of the response. Comment on what you observe and how it will effect your analysis. hist(salamanders$count) Based on the histogram, there are many 0s. When there are too many 0s, there could be two distributions. The first is a hurdle from 0 to non-zero. Then the second distribution could be for non-zero responses. Fit a mixed model to the data. You are free to choose how to conduct the analysis. Please specifically list the following choices along with a justification. • (4 points) R package (lme4, MCMCglmm, glmmTMB, etc) I will use glmmTMB as it has the flexibity of fitting zero inflated and hurdle model for negative bionomial with random and fixed effects. • (5 points) Response distribution. To model the count data with many 0s, I will use zero-inflated negative bionomial regression. • (7 points) Model structure (fixed and random effects, etc) #??? does the fixed effect also need to be nested? vs. fixed effect analysis? Site is random effect since I am not interested in the specific location. There could be more locations to be sampled. The species are fixed effect since I am interested in the specific type’s impact on the count. (6 points) Conduct an analysis of your fixed effects and comment on how it answers the questions of interest. library(glmmTMB) sal_glmm_zi &lt;- glmmTMB(count ~ 1 + mined*spp + (1|site), ziformula=~1,family=nbinom2(link = &quot;log&quot;), data=salamanders) #??? S26 GLMMs - why set parameter = ~1, ~0, #??? S35 GLMMs count data. how are these diff? glmmTMB, zeroinfl. what&#39;s the scale? log? #??? S46 Randomeffect -&gt; class nested in school? math, english why? or unique class? summary(sal_glmm_zi) ## Family: nbinom2 ( log ) ## Formula: count ~ 1 + mined * spp + (1 | site) ## Zero inflation: ~1 ## Data: salamanders ## ## AIC BIC logLik deviance df.resid ## 1665.3 1741.3 -815.7 1631.3 627 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## site (Intercept) 0.2827 0.5317 ## Number of obs: 644, groups: site, 23 ## ## Dispersion parameter for nbinom2 family (): 1.04 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.3739 0.2429 5.657 1.54e-08 *** ## minedyes -2.2238 0.3954 -5.624 1.87e-08 *** ## sppDF -0.7417 0.2481 -2.990 0.00279 ** ## sppDM -0.4769 0.2413 -1.976 0.04813 * ## sppEC-A -1.4398 0.2705 -5.324 1.02e-07 *** ## sppEC-L -0.0993 0.2377 -0.418 0.67619 ## sppGP -0.5779 0.2422 -2.386 0.01701 * ## sppPR -2.2247 0.3109 -7.155 8.34e-13 *** ## minedyes:sppDF 0.8066 0.4521 1.784 0.07442 . ## minedyes:sppDM 0.2157 0.4633 0.466 0.64156 ## minedyes:sppEC-A -0.3567 0.6537 -0.546 0.58531 ## minedyes:sppEC-L -0.6005 0.5025 -1.195 0.23208 ## minedyes:sppGP -1.9324 0.8159 -2.368 0.01786 * ## minedyes:sppPR 0.6446 0.6331 1.018 0.30862 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Zero-inflation model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.354 7.862 -0.554 0.58 The fixed effect analysis show that: Mining has a negative and significant simple effect on the number of salamanders (-2.22), p &lt; 0.01. Mining’s effect depends on the specie. The GP specie has a negative interaction with the mining. When there is minig, the GP specie’s count decrease is much higher (-1.93), p&lt;0.05 (3 points) Create one or more diagnostic plots and comment. qqnorm(residuals(sal_glmm_zi),main=&quot;QQ plot of Residuals&quot;) qqline(residuals(sal_glmm_zi)) The model doesn’t fit well as the two ends deviate from the normal line. #??? how to fix? is it the outlier? #??? how to interpret? S50 random effect ppt plot(fitted(sal_glmm_zi),residuals(sal_glmm_zi),xlab=&quot;Fitted&quot;,ylab=&quot;Residuals&quot;,main=&quot;Residuals vs Fitted&quot;) abline(h=0) #qqnorm(ranef(sal_glmm_zi)$site[[1]],main=&quot;site Effects&quot;) #qqline(ranef(sal_glmm_zi)$site[[1]]) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
